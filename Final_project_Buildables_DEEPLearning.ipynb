{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVEIgeEsE9_Y",
        "outputId": "0a7f9d62-e9c6-48b8-eb7d-aceb52c48171"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 2.19.0\n",
            "GPU device: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "# Cell 1 — verify GPU\n",
        "import tensorflow as tf\n",
        "print(\"TensorFlow\", tf.__version__)\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print(\"GPU device:\", device_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXX5OIk3KVzw",
        "outputId": "5f8815b1-642b-4dc8-8deb-07e9af7dd2a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------\n",
        "# 2. Search the whole Drive for the zip file\n",
        "# -------------------------------------------------\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "zip_name = \"plant_disease.zip\"\n",
        "\n",
        "# Walk through the mounted Drive\n",
        "found_path = None\n",
        "for root, dirs, files in os.walk(\"/content/drive/MyDrive\"):\n",
        "    if zip_name in files:\n",
        "        found_path = Path(root) / zip_name\n",
        "        break\n",
        "\n",
        "if found_path is None:\n",
        "    print(f\"File '{zip_name}' not found in My Drive.\")\n",
        "else:\n",
        "    print(f\"Found at: {found_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAFgxCjjN2Th",
        "outputId": "e3d2275d-a488-4652-c6f1-2eefbce40050"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found at: /content/drive/MyDrive/Buildable-ML-DL-Fellowship/plant_disease.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab cell — robust Drive dataset setup (copy & run)\n",
        "from google.colab import drive\n",
        "import os, shutil, sys, textwrap\n",
        "\n",
        "# 1) mount drive (if not already)\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "# 2) base folder in your Drive (change only if you used a different folder)\n",
        "DRIVE_BASE = '/content/drive/MyDrive/Buildable-ML-DL-Fellowship'\n",
        "print(\"DRIVE_BASE =\", DRIVE_BASE)\n",
        "\n",
        "# 3) candidate paths to search for the dataset\n",
        "candidates = [\n",
        "    os.path.join(DRIVE_BASE, 'plant_disease.zip'),              # MyDrive/.../plant_disease.zip\n",
        "    os.path.join(DRIVE_BASE, 'data', 'plant_disease.zip'),     # MyDrive/.../data/plant_disease.zip\n",
        "    os.path.join(DRIVE_BASE, 'data', 'plant_disease'),         # MyDrive/.../data/plant_disease (folder)\n",
        "    os.path.join(DRIVE_BASE, 'plant_disease'),                 # MyDrive/.../plant_disease (folder)\n",
        "]\n",
        "\n",
        "print(\"\\nListing DRIVE_BASE contents (first 200 entries):\")\n",
        "try:\n",
        "    for i,entry in enumerate(os.listdir(DRIVE_BASE)[:200]):\n",
        "        print(\" \", entry)\n",
        "except FileNotFoundError:\n",
        "    print(\"Drive base folder not found. Maybe you used a different Drive path.\")\n",
        "    print(\"Please upload your dataset to your Drive under MyDrive/Buildable-ML-DL-Fellowship and re-run.\")\n",
        "    raise\n",
        "\n",
        "# 4) set where we will put the dataset in the Colab filesystem\n",
        "DATA_DIR = '/content/plant_disease'\n",
        "MODELS_DIR = os.path.join(DRIVE_BASE, 'models')\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "\n",
        "# 5) find the dataset and extract/copy it\n",
        "found = False\n",
        "for cand in candidates:\n",
        "    if os.path.exists(cand):\n",
        "        found = True\n",
        "        if cand.endswith('.zip'):\n",
        "            print(f\"\\nFound zip at: {cand}\\nUnzipping to {DATA_DIR} ...\")\n",
        "            # remove old DATA_DIR and unzip\n",
        "            if os.path.exists(DATA_DIR):\n",
        "                shutil.rmtree(DATA_DIR)\n",
        "            !unzip -q \"{cand}\" -d \"{DATA_DIR}\"\n",
        "            print(\"Unzip completed.\")\n",
        "        else:\n",
        "            # it's a folder, copy it to /content\n",
        "            print(f\"\\nFound folder at: {cand}\\nCopying to {DATA_DIR} ...\")\n",
        "            if os.path.exists(DATA_DIR):\n",
        "                shutil.rmtree(DATA_DIR)\n",
        "            shutil.copytree(cand, DATA_DIR)\n",
        "            print(\"Copy completed.\")\n",
        "        break\n",
        "\n",
        "if not found:\n",
        "    print(\"\\nNo dataset found in expected locations.\")\n",
        "    print(textwrap.dedent(f'''\n",
        "        Please check these locations in your Google Drive:\n",
        "          {DRIVE_BASE}/plant_disease.zip\n",
        "          {DRIVE_BASE}/data/plant_disease.zip\n",
        "          {DRIVE_BASE}/data/plant_disease\n",
        "          {DRIVE_BASE}/plant_disease\n",
        "\n",
        "        Use Google Drive (https://drive.google.com) to confirm where you uploaded the zip/folder,\n",
        "        then either:\n",
        "         - Move the zip into: MyDrive/Buildable-ML-DL-Fellowship/plant_disease.zip\n",
        "         - OR create a folder: MyDrive/Buildable-ML-DL-Fellowship/plant_disease/ and put train/val/test inside it.\n",
        "\n",
        "        Alternatively, upload directly from your local machine into this Colab session (not Drive) with:\n",
        "          from google.colab import files\n",
        "          uploaded = files.upload()\n",
        "        (but then you'll need to handle placement/structure manually)\n",
        "    '''))\n",
        "    raise FileNotFoundError(\"Dataset not found in Drive.\")\n",
        "\n",
        "# 6) final check: print structure preview\n",
        "print(\"\\nDATA_DIR set to:\", DATA_DIR)\n",
        "for split in ['train','val','test']:\n",
        "    p = os.path.join(DATA_DIR, split)\n",
        "    if os.path.exists(p):\n",
        "        classes = [d for d in os.listdir(p) if os.path.isdir(os.path.join(p,d))]\n",
        "        print(f\"  {split}: {len(classes)} classes, e.g. {classes[:10]}\")\n",
        "    else:\n",
        "        print(f\"  WARNING: {split} directory not found under {DATA_DIR} — check your dataset structure.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KGzBU4QHbYS",
        "outputId": "d0b29a18-5368-49ab-c9f9-b770d36c81fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "DRIVE_BASE = /content/drive/MyDrive/Buildable-ML-DL-Fellowship\n",
            "\n",
            "Listing DRIVE_BASE contents (first 200 entries):\n",
            "  plant_disease.zip\n",
            "  models\n",
            "\n",
            "Found zip at: /content/drive/MyDrive/Buildable-ML-DL-Fellowship/plant_disease.zip\n",
            "Unzipping to /content/plant_disease ...\n",
            "Unzip completed.\n",
            "\n",
            "DATA_DIR set to: /content/plant_disease\n",
            "  WARNING: train directory not found under /content/plant_disease — check your dataset structure.\n",
            "  WARNING: val directory not found under /content/plant_disease — check your dataset structure.\n",
            "  WARNING: test directory not found under /content/plant_disease — check your dataset structure.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab cell — inspect and create train/val/test splits if missing\n",
        "import os, shutil, random\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "BASE = Path('/content/plant_disease')   # where we unzipped\n",
        "print(\"BASE exists:\", BASE.exists())\n",
        "print(\"BASE listing (top level):\")\n",
        "print(sorted([p.name for p in BASE.iterdir()]))\n",
        "\n",
        "# helper to check if a directory contains image files\n",
        "IMG_EXTS = {'.jpg','.jpeg','.png','.bmp','.tif','.tiff'}\n",
        "def contains_images(p: Path):\n",
        "    if not p.exists() or not p.is_dir(): return False\n",
        "    for ff in p.iterdir():\n",
        "        if ff.is_file() and ff.suffix.lower() in IMG_EXTS:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "# 1) Search for existing train/val/test anywhere and copy if found\n",
        "def find_and_copy_existing_splits(base: Path):\n",
        "    for root, dirs, files in os.walk(base):\n",
        "        # check if this root has train/val/test subdirs\n",
        "        subdirs = set(dirs)\n",
        "        if {'train','val','test'}.issubset(subdirs):\n",
        "            src = Path(root)\n",
        "            print(\"Found existing split structure at:\", src)\n",
        "            # copy each to top-level /content/plant_disease/train etc\n",
        "            dest_base = base\n",
        "            # remove old if exists\n",
        "            for s in ['train','val','test']:\n",
        "                d = dest_base / s\n",
        "                if d.exists():\n",
        "                    shutil.rmtree(d)\n",
        "                shutil.copytree(src / s, d)\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "# 2) If not found, try to detect class-root folder(s)\n",
        "def find_class_root(base: Path):\n",
        "    # Option A: top-level subdir that itself contains class subdirs with images\n",
        "    for child in base.iterdir():\n",
        "        if child.is_dir():\n",
        "            # check if child contains many subdirs each with images\n",
        "            child_subdirs = [d for d in child.iterdir() if d.is_dir()]\n",
        "            if child_subdirs and any(contains_images(d) for d in child_subdirs):\n",
        "                return child\n",
        "    # Option B: base itself may have class subdirs (each containing images)\n",
        "    if any(contains_images(base / d) for d in os.listdir(base) if (base/d).is_dir()):\n",
        "        return base\n",
        "    # Option C: deeper search: find first folder containing multiple class subfolders with images\n",
        "    for root, dirs, files in os.walk(base):\n",
        "        candidate = Path(root)\n",
        "        subdirs = [candidate / d for d in dirs]\n",
        "        if len(subdirs) >= 2 and any(contains_images(sd) for sd in subdirs):\n",
        "            return candidate\n",
        "    return None\n",
        "\n",
        "# 3) Create splits from detected class root\n",
        "def create_splits_from_classroot(class_root: Path, dest_base: Path, seed=42, ratios=(0.7,0.15,0.15)):\n",
        "    print(\"Creating splits from class root:\", class_root)\n",
        "    dest_train = dest_base / 'train'\n",
        "    dest_val   = dest_base / 'val'\n",
        "    dest_test  = dest_base / 'test'\n",
        "    # remove existing dest folders if present\n",
        "    for d in [dest_train, dest_val, dest_test]:\n",
        "        if d.exists():\n",
        "            shutil.rmtree(d)\n",
        "        d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    classes = [d for d in sorted(class_root.iterdir()) if d.is_dir()]\n",
        "    print(\"Detected class folders (first 30):\", [c.name for c in classes[:30]])\n",
        "    for class_dir in classes:\n",
        "        imgs = [p for p in class_dir.rglob('*') if p.is_file() and p.suffix.lower() in IMG_EXTS]\n",
        "        imgs = sorted(imgs)\n",
        "        if not imgs:\n",
        "            print(\"  (skip) no images for class\", class_dir.name)\n",
        "            continue\n",
        "        # split\n",
        "        train_and_temp, test_files = train_test_split(imgs, test_size=ratios[2], random_state=seed)\n",
        "        val_size_rel = ratios[1] / (ratios[0] + ratios[1])\n",
        "        train_files, val_files = train_test_split(train_and_temp, test_size=val_size_rel, random_state=seed)\n",
        "        # copy\n",
        "        (dest_train / class_dir.name).mkdir(parents=True, exist_ok=True)\n",
        "        (dest_val / class_dir.name).mkdir(parents=True, exist_ok=True)\n",
        "        (dest_test / class_dir.name).mkdir(parents=True, exist_ok=True)\n",
        "        for src in train_files:\n",
        "            dst = dest_train / class_dir.name / src.name\n",
        "            shutil.copy2(src, dst)\n",
        "        for src in val_files:\n",
        "            dst = dest_val / class_dir.name / src.name\n",
        "            shutil.copy2(src, dst)\n",
        "        for src in test_files:\n",
        "            dst = dest_test / class_dir.name / src.name\n",
        "            shutil.copy2(src, dst)\n",
        "        print(f\"  {class_dir.name}: train={len(train_files)} val={len(val_files)} test={len(test_files)}\")\n",
        "    print(\"Done creating splits at\", dest_base)\n",
        "\n",
        "# Run the detection & splitting logic\n",
        "base = BASE\n",
        "if not base.exists():\n",
        "    raise FileNotFoundError(f\"{base} not found in Colab filesystem. Please check unzip earlier.\")\n",
        "\n",
        "# step A: try to find existing splits and copy to top-level\n",
        "found_splits = find_and_copy_existing_splits(base)\n",
        "if found_splits:\n",
        "    print(\"Copied existing splits into top-level train/val/test.\")\n",
        "else:\n",
        "    # step B: find class root\n",
        "    class_root = find_class_root(base)\n",
        "    if class_root is None:\n",
        "        print(\"No class root found automatically. Directory tree (two levels):\")\n",
        "        for root, dirs, files in os.walk(base):\n",
        "            print(root, \"->\", len(dirs), \"dirs,\", len(files), \"files\")\n",
        "        raise RuntimeError(\"Could not automatically find class folders. Please check the zip structure in Drive.\")\n",
        "    # create splits from class_root\n",
        "    create_splits_from_classroot(class_root, base, seed=42, ratios=(0.7,0.15,0.15))\n",
        "\n",
        "# Finally, print summary counts per split\n",
        "for split in ['train','val','test']:\n",
        "    sdir = base / split\n",
        "    if not sdir.exists():\n",
        "        print(f\"WARNING: {split} does not exist!\")\n",
        "        continue\n",
        "    classes = [d for d in sdir.iterdir() if d.is_dir()]\n",
        "    print(f\"\\nSplit {split}: {len(classes)} classes\")\n",
        "    for c in classes[:40]:\n",
        "        cnt = len(list(c.glob('*')))\n",
        "        print(f\"  {c.name}: {cnt}\")\n",
        "print(\"\\nSplitting complete. You can now run the image_dataset_from_directory cell.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxtpfz46Rhih",
        "outputId": "c618c481-e9d3-43b7-e93d-d64c87f3c8f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BASE exists: True\n",
            "BASE listing (top level):\n",
            "['plant_disease']\n",
            "Found existing split structure at: /content/plant_disease/plant_disease\n",
            "Copied existing splits into top-level train/val/test.\n",
            "\n",
            "Split train: 1 classes\n",
            "  PlantVillage: 18785\n",
            "\n",
            "Split val: 1 classes\n",
            "  PlantVillage: 5734\n",
            "\n",
            "Split test: 1 classes\n",
            "  PlantVillage: 5758\n",
            "\n",
            "Splitting complete. You can now run the image_dataset_from_directory cell.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1 — inspect nested structure\n",
        "from pathlib import Path\n",
        "BASE = Path('/content/plant_disease')\n",
        "for split in ['train','val','test']:\n",
        "    p = BASE / split\n",
        "    print(f\"\\n=== {p} === (exists={p.exists()})\")\n",
        "    if not p.exists():\n",
        "        continue\n",
        "    level1 = sorted([x for x in p.iterdir() if x.is_dir()])\n",
        "    print(\" Level-1 dirs:\", [d.name for d in level1][:40])\n",
        "    # If there's exactly one folder at level1, inspect its subfolders\n",
        "    if len(level1) == 1:\n",
        "        child = level1[0]\n",
        "        sub = sorted([x for x in child.iterdir() if x.is_dir()])\n",
        "        print(\"  Found single wrapper folder:\", child.name)\n",
        "        print(\"  Its subdirs (first 40):\", [d.name for d in sub][:40])\n",
        "        # count images in first few subdirs\n",
        "        for d in sub[:10]:\n",
        "            cnt = len([f for f in d.rglob('*') if f.suffix.lower() in {'.jpg','.jpeg','.png','.bmp'}])\n",
        "            print(f\"    {d.name}: {cnt} images\")\n",
        "    else:\n",
        "        # show per-level1 counts\n",
        "        for d in level1[:40]:\n",
        "            cnt = len([f for f in d.rglob('*') if f.suffix.lower() in {'.jpg','.jpeg','.png','.bmp'}])\n",
        "            print(f\"  {d.name}: {cnt} images\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dit1Sr54O_y5",
        "outputId": "6ec85da8-83d0-4f3f-9813-1031d92b9f31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== /content/plant_disease/train === (exists=True)\n",
            " Level-1 dirs: ['PlantVillage']\n",
            "  Found single wrapper folder: PlantVillage\n",
            "  Its subdirs (first 40): []\n",
            "\n",
            "=== /content/plant_disease/val === (exists=True)\n",
            " Level-1 dirs: ['PlantVillage']\n",
            "  Found single wrapper folder: PlantVillage\n",
            "  Its subdirs (first 40): []\n",
            "\n",
            "=== /content/plant_disease/test === (exists=True)\n",
            " Level-1 dirs: ['PlantVillage']\n",
            "  Found single wrapper folder: PlantVillage\n",
            "  Its subdirs (first 40): []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2 — flatten wrapper folder if each split contains a single wrapper folder\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "BASE = Path('/content/plant_disease')\n",
        "for split in ['train','val','test']:\n",
        "    p = BASE / split\n",
        "    if not p.exists():\n",
        "        print(f\"Skip {split}: not present\")\n",
        "        continue\n",
        "    children = [d for d in p.iterdir() if d.is_dir()]\n",
        "    if len(children) == 1:\n",
        "        wrapper = children[0]\n",
        "        print(f\"Flattening {split}: wrapper {wrapper.name} -> moving its subfolders up to {p}\")\n",
        "        subs = [d for d in wrapper.iterdir() if d.is_dir()]\n",
        "        for s in subs:\n",
        "            dest = p / s.name\n",
        "            if dest.exists():\n",
        "                print(f\"  Destination exists, skipping move of {s} -> {dest}\")\n",
        "            else:\n",
        "                shutil.move(str(s), str(dest))\n",
        "                print(f\"  Moved {s.name}\")\n",
        "        # If wrapper is now empty of subdirs, remove it (but keep files if any)\n",
        "        try:\n",
        "            # remove wrapper if empty\n",
        "            if not any(wrapper.iterdir()):\n",
        "                wrapper.rmdir()\n",
        "                print(f\"  Removed empty wrapper folder {wrapper}\")\n",
        "            else:\n",
        "                print(f\"  Wrapper {wrapper} still contains files (not removed).\")\n",
        "        except Exception as e:\n",
        "            print(\"  Could not remove wrapper:\", e)\n",
        "    else:\n",
        "        print(f\"No single wrapper for {split} (has {len(children)} top-level folders).\")\n",
        "\n",
        "# print final top-level per-split directories\n",
        "print(\"\\nAfter flattening, top-level contents:\")\n",
        "for split in ['train','val','test']:\n",
        "    p = BASE / split\n",
        "    if p.exists():\n",
        "        print(split, \"->\", sorted([d.name for d in p.iterdir() if d.is_dir()])[:40])\n",
        "    else:\n",
        "        print(split, \"missing\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4pKiCITSFOB",
        "outputId": "73cd932d-9715-4906-dd3e-c890db9d9b83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flattening train: wrapper PlantVillage -> moving its subfolders up to /content/plant_disease/train\n",
            "  Wrapper /content/plant_disease/train/PlantVillage still contains files (not removed).\n",
            "Flattening val: wrapper PlantVillage -> moving its subfolders up to /content/plant_disease/val\n",
            "  Wrapper /content/plant_disease/val/PlantVillage still contains files (not removed).\n",
            "Flattening test: wrapper PlantVillage -> moving its subfolders up to /content/plant_disease/test\n",
            "  Wrapper /content/plant_disease/test/PlantVillage still contains files (not removed).\n",
            "\n",
            "After flattening, top-level contents:\n",
            "train -> ['PlantVillage']\n",
            "val -> ['PlantVillage']\n",
            "test -> ['PlantVillage']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3 — create datasets using image_dataset_from_directory\n",
        "import tensorflow as tf, json, os\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_DIR = '/content/plant_disease'\n",
        "IMG_SIZE = (224,224)\n",
        "BATCH_SIZE = 32\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_dir = os.path.join(DATA_DIR, 'train')\n",
        "val_dir = os.path.join(DATA_DIR, 'val')\n",
        "test_dir = os.path.join(DATA_DIR, 'test')\n",
        "\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    train_dir, labels='inferred', label_mode='categorical',\n",
        "    image_size=IMG_SIZE, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    val_dir, labels='inferred', label_mode='categorical',\n",
        "    image_size=IMG_SIZE, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    test_dir, labels='inferred', label_mode='categorical',\n",
        "    image_size=IMG_SIZE, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "class_names = train_ds.class_names\n",
        "print(\"Detected classes:\", len(class_names), class_names[:40])\n",
        "# Save class mapping\n",
        "MODELS_DIR = '/content/drive/MyDrive/Buildable-ML-DL-Fellowship/models'\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "with open(os.path.join(MODELS_DIR,'classes.json'),'w') as f:\n",
        "    json.dump(class_names, f)\n",
        "\n",
        "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "# basic augmentation layer\n",
        "data_augmentation = keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.05),\n",
        "    layers.RandomZoom(0.05),\n",
        "], name='data_augmentation')\n",
        "print(\"Datasets ready. train batches:\", tf.data.experimental.cardinality(train_ds).numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GstTIgJVSFLn",
        "outputId": "a27c4ef6-5fe8-4362-f975-c14dccb3f05c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 18785 files belonging to 1 classes.\n",
            "Found 5734 files belonging to 1 classes.\n",
            "Found 5758 files belonging to 1 classes.\n",
            "Detected classes: 1 ['PlantVillage']\n",
            "Datasets ready. train batches: 588\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4 — baseline CNN model definition & compile\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "def make_baseline_cnn(input_shape=IMG_SIZE+(3,), num_classes=len(class_names)):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = data_augmentation(inputs)\n",
        "    x = layers.Rescaling(1./255)(x)\n",
        "    x = layers.Conv2D(32,3,activation='relu',padding='same')(x)\n",
        "    x = layers.MaxPool2D()(x)\n",
        "    x = layers.Conv2D(64,3,activation='relu',padding='same')(x)\n",
        "    x = layers.MaxPool2D()(x)\n",
        "    x = layers.Conv2D(128,3,activation='relu',padding='same')(x)\n",
        "    x = layers.MaxPool2D()(x)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    x = layers.Dense(128, activation='relu')(x)\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "    model = keras.Model(inputs, outputs, name='baseline_cnn')\n",
        "    return model\n",
        "\n",
        "baseline = make_baseline_cnn()\n",
        "baseline.compile(optimizer=keras.optimizers.Adam(1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "baseline.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "id": "3Dm1S5maSFJO",
        "outputId": "cfd54742-08ca-4dd2-9adf-9d16e7e947f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"baseline_cnn\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"baseline_cnn\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ data_augmentation (\u001b[38;5;33mSequential\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ rescaling (\u001b[38;5;33mRescaling\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m16,512\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ data_augmentation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ rescaling (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m109,889\u001b[0m (429.25 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">109,889</span> (429.25 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m109,889\u001b[0m (429.25 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">109,889</span> (429.25 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5 — baseline training\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "baseline_ckpt = os.path.join(MODELS_DIR, 'disease_baseline_best.h5')\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True),\n",
        "    ModelCheckpoint(baseline_ckpt, monitor='val_loss', save_best_only=True),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
        "]\n",
        "history_baseline = baseline.fit(train_ds, validation_data=val_ds, epochs=20, callbacks=callbacks)\n",
        "baseline.save(os.path.join(MODELS_DIR,'disease_baseline.h5'))\n",
        "\n",
        "# save plots\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_history(h, name):\n",
        "    plt.figure(); plt.plot(h.history['loss'], label='train_loss'); plt.plot(h.history['val_loss'], label='val_loss'); plt.legend(); plt.title(name+' loss'); plt.savefig(os.path.join(MODELS_DIR,name+'_loss.png')); plt.close()\n",
        "    if 'accuracy' in h.history:\n",
        "        plt.figure(); plt.plot(h.history['accuracy'], label='train_acc'); plt.plot(h.history['val_accuracy'], label='val_acc'); plt.legend(); plt.title(name+' acc'); plt.savefig(os.path.join(MODELS_DIR,name+'_acc.png')); plt.close()\n",
        "plot_history(history_baseline, 'baseline')\n",
        "print(\"Saved baseline model and plots to\", MODELS_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAKPGMieSFGe",
        "outputId": "d6b8a52c-bb92-4396-e8c6-54f731449fa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/ops/nn.py:944: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/losses/losses.py:33: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
            "  return self.fn(y_true, y_pred, **self._fn_kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 0.0000e+00"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 56ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00 - learning_rate: 0.0010\n",
            "Epoch 2/20\n",
            "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 59ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00 - learning_rate: 0.0010\n",
            "Epoch 3/20\n",
            "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 55ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00 - learning_rate: 0.0010\n",
            "Epoch 4/20\n",
            "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00 - learning_rate: 0.0010\n",
            "Epoch 5/20\n",
            "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 58ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00 - learning_rate: 5.0000e-04\n",
            "Epoch 6/20\n",
            "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00 - learning_rate: 5.0000e-04\n",
            "Epoch 7/20\n",
            "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 55ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00 - learning_rate: 5.0000e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved baseline model and plots to /content/drive/MyDrive/Buildable-ML-DL-Fellowship/models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6 — ResNet50 transfer learning model creation\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "def make_resnet_model(input_shape=IMG_SIZE+(3,), num_classes=len(class_names), base_trainable=False):\n",
        "    base = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape, pooling='avg')\n",
        "    base.trainable = base_trainable\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = data_augmentation(inputs)\n",
        "    x = layers.Rescaling(1./255)(x)\n",
        "    x = base(x, training=False)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    x = layers.Dense(256, activation='relu')(x)\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "    model = keras.Model(inputs, outputs, name='resnet50_transfer')\n",
        "    return model\n",
        "\n",
        "resnet_model = make_resnet_model(base_trainable=False)\n",
        "resnet_model.compile(optimizer=keras.optimizers.Adam(1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "resnet_model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "YU2aOlpqVSfH",
        "outputId": "bdc0b093-477d-424b-987d-79cb57c6f8ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"resnet50_transfer\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"resnet50_transfer\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_3 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ data_augmentation (\u001b[38;5;33mSequential\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ rescaling_1 (\u001b[38;5;33mRescaling\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ resnet50 (\u001b[38;5;33mFunctional\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │    \u001b[38;5;34m23,587,712\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m524,544\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m257\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ data_augmentation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ rescaling_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ resnet50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │    <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,544</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m24,112,513\u001b[0m (91.98 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,112,513</span> (91.98 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m524,801\u001b[0m (2.00 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">524,801</span> (2.00 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m23,587,712\u001b[0m (89.98 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> (89.98 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7 — train top layers then fine-tune\n",
        "resnet_ckpt = os.path.join(MODELS_DIR, 'disease_resnet50_top_best.h5')\n",
        "callbacks = [EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True),\n",
        "             ModelCheckpoint(resnet_ckpt, monitor='val_loss', save_best_only=True),\n",
        "             ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)]\n",
        "\n",
        "history_resnet_top = resnet_model.fit(train_ds, validation_data=val_ds, epochs=10, callbacks=callbacks)\n",
        "resnet_model.save(os.path.join(MODELS_DIR,'disease_resnet50_top.h5'))\n",
        "plot_history(history_resnet_top, 'resnet_top')\n",
        "\n",
        "# find base and unfreeze last N layers\n",
        "base_model = None\n",
        "for layer in resnet_model.layers:\n",
        "    if 'resnet50' in layer.name:\n",
        "        base_model = layer\n",
        "        break\n",
        "if base_model is None:\n",
        "    base_model = resnet_model.layers[3]\n",
        "\n",
        "for layer in base_model.layers[:-20]:\n",
        "    layer.trainable = False\n",
        "for layer in base_model.layers[-20:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "resnet_model.compile(optimizer=keras.optimizers.Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history_resnet_ft = resnet_model.fit(train_ds, validation_data=val_ds, epochs=10, callbacks=[\n",
        "    EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True),\n",
        "    ModelCheckpoint(os.path.join(MODELS_DIR,'disease_resnet50_ft_best.h5'), monitor='val_loss', save_best_only=True),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
        "])\n",
        "resnet_model.save(os.path.join(MODELS_DIR,'disease_resnet50.h5'))\n",
        "plot_history(history_resnet_ft, 'resnet_finetune')\n",
        "print(\"Saved ResNet models + curves to\", MODELS_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5Ry7nR4SFEA",
        "outputId": "b9d0b181-db0c-4076-c5c8-bc5bd2d9ed3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 1.0000 - loss: 0.0000e+00"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 173ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 166ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 198ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 167ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 166ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00 - learning_rate: 5.0000e-04\n",
            "Epoch 6/10\n",
            "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 199ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00 - learning_rate: 5.0000e-04\n",
            "Epoch 7/10\n",
            "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 167ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00 - learning_rate: 5.0000e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 1.0000 - loss: 0.0000e+00"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 232ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00 - learning_rate: 1.0000e-05\n",
            "Epoch 2/10\n",
            "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 194ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00 - learning_rate: 1.0000e-05\n",
            "Epoch 3/10\n",
            "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 193ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00 - learning_rate: 1.0000e-05\n",
            "Epoch 4/10\n",
            "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 193ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00 - learning_rate: 1.0000e-05\n",
            "Epoch 5/10\n",
            "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 194ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00 - learning_rate: 5.0000e-06\n",
            "Epoch 6/10\n",
            "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 192ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00 - learning_rate: 5.0000e-06\n",
            "Epoch 7/10\n",
            "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 193ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00 - learning_rate: 5.0000e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved ResNet models + curves to /content/drive/MyDrive/Buildable-ML-DL-Fellowship/models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8 — evaluation & inference time\n",
        "import time, json\n",
        "from tensorflow.keras.models import load_model\n",
        "resnet_path = os.path.join(MODELS_DIR,'disease_resnet50.h5')\n",
        "if os.path.exists(resnet_path):\n",
        "    model = load_model(resnet_path)\n",
        "else:\n",
        "    model = resnet_model\n",
        "\n",
        "loss, acc = model.evaluate(test_ds)\n",
        "print(\"Test loss, acc:\", loss, acc)\n",
        "\n",
        "# inference timing on one batch\n",
        "batch = next(iter(test_ds))\n",
        "images, labels = batch\n",
        "n_runs = 50\n",
        "t0 = time.perf_counter()\n",
        "for _ in range(n_runs):\n",
        "    _ = model.predict(images, verbose=0)\n",
        "t1 = time.perf_counter()\n",
        "avg_ms_per_image = ((t1 - t0) / n_runs) * 1000.0 / images.shape[0]\n",
        "metrics = {'test_loss': float(loss), 'test_acc': float(acc), 'avg_inference_ms_per_image': float(avg_ms_per_image)}\n",
        "with open(os.path.join(MODELS_DIR,'disease_metrics.json'),'w') as f:\n",
        "    json.dump(metrics, f)\n",
        "print(\"Avg ms per image:\", avg_ms_per_image, \"Saved metrics to\", os.path.join(MODELS_DIR,'disease_metrics.json'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0VpVMlGSFBy",
        "outputId": "a5414c8c-bebf-4288-8d36-d171aa2c16b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 128ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
            "Test loss, acc: 0.0 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/ops/nn.py:944: UserWarning: You are using a softmax over axis -1 of a tensor of shape (32, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg ms per image: 6.756160526250028 Saved metrics to /content/drive/MyDrive/Buildable-ML-DL-Fellowship/models/disease_metrics.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect sample files / folders\n",
        "from pathlib import Path\n",
        "BASE = Path('/content/plant_disease')\n",
        "for split in ['train','val','test']:\n",
        "    p = BASE / split\n",
        "    print(f\"\\n== {split} ==\")\n",
        "    if not p.exists():\n",
        "        print(\"  MISSING\", p); continue\n",
        "    children = [d for d in p.iterdir() if d.is_dir()]\n",
        "    print(\"  top-level dirs:\", [d.name for d in children][:10])\n",
        "    # if a single wrapper folder exists, list inside it\n",
        "    if len(children) == 1:\n",
        "        wrapper = children[0]\n",
        "        print(\"  wrapper folder:\", wrapper.name)\n",
        "        wrapper_children = [x for x in wrapper.iterdir()]\n",
        "        # show 20 entries (files or folders)\n",
        "        for e in wrapper_children[:20]:\n",
        "            print(\"   \", (\"DIR \" if e.is_dir() else \"FILE\"), e.name)\n",
        "        # sample file names (if files exist directly)\n",
        "        sample_files = [f for f in wrapper.rglob('*') if f.is_file()]\n",
        "        print(\"  sample filenames (first 20):\")\n",
        "        for s in sample_files[:20]:\n",
        "            print(\"   \", s.relative_to(BASE))\n",
        "    else:\n",
        "        # show sample files at this level\n",
        "        sample_files = [f for f in p.rglob('*') if f.is_file()]\n",
        "        print(\"  sample filenames (first 20):\")\n",
        "        for s in sample_files[:20]:\n",
        "            print(\"   \", s.relative_to(BASE))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7cQ0LtGe2_2",
        "outputId": "4cc02aa6-d1e1-41d8-80da-ec0e47e0c70b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "== train ==\n",
            "  top-level dirs: ['PlantVillage']\n",
            "  wrapper folder: PlantVillage\n",
            "    FILE 829d0a5e-326c-4ae6-a3c6-c65297cb5d2f___RS_Erly.B 8338.JPG\n",
            "    FILE dbb6495b-0765-4d94-afad-6020819aa893___JR_HL 8233.JPG\n",
            "    FILE f536e055-666e-41cc-8ee2-c1af2fbf754a___RS_Early.B 8104.JPG\n",
            "    FILE 6173dbe2-1379-4241-a43e-7cbad448b6cc___RS_Early.B 6993.JPG\n",
            "    FILE ffcfa32d-4506-49ed-9e35-972d5cbad0b8___Com.G_SpM_FL 8450.JPG\n",
            "    FILE 08f0fd0e-d4b1-400d-9df1-2d61d24c95f0___Com.G_SpM_FL 9397.JPG\n",
            "    FILE 7c9ac47e-ec96-457c-96d7-8aefdace978f___Crnl_L.Mold 6850.JPG\n",
            "    FILE 7cf516ee-327e-40a6-810b-7c08ea57f825___UF.GRC_YLCV_Lab 02062.JPG\n",
            "    FILE fc6a6a03-757b-4549-8c2e-194141456f60___GCREC_Bact.Sp 6252.JPG\n",
            "    FILE 0ce74db6-be9b-4c43-a104-6a3f9bcd2de2___NREC_B.Spot 1827.JPG\n",
            "    FILE 9ea77bf9-c078-4adf-9173-ca0dca15dabd___GCREC_Bact.Sp 6333.JPG\n",
            "    FILE 71048e39-6d6e-4381-9113-f2b4bd71ab93___Com.G_SpM_FL 8749.JPG\n",
            "    FILE b7fb98ad-68d6-46b9-9f17-9d3f111b20fb___Com.G_SpM_FL 1676.JPG\n",
            "    FILE 503c6a57-2af3-48ed-b96f-e2cbf6fb5dd1___PSU_CG 2263.JPG\n",
            "    FILE badeca64-87ed-439a-b373-b4b872dbe173___GCREC_Bact.Sp 6219.JPG\n",
            "    FILE 447735c7-b650-4ef6-a53b-8fb661470de3___JR_B.Spot 3370.JPG\n",
            "    FILE 58adb74c-a1b3-4b9d-95b7-f0721c499388___RS_LB 4326.JPG\n",
            "    FILE a2664ab8-aca6-4891-9f6f-8cc414d6b231___JR_HL 5849.JPG\n",
            "    FILE b8b7b98a-eb1a-4213-9b0b-aeef4df427e8___RS_HL 1858.JPG\n",
            "    FILE e7619088-871f-422e-a6e8-5c8a7da70b3e___GHLB2 Leaf 8825.JPG\n",
            "  sample filenames (first 20):\n",
            "    train/PlantVillage/829d0a5e-326c-4ae6-a3c6-c65297cb5d2f___RS_Erly.B 8338.JPG\n",
            "    train/PlantVillage/dbb6495b-0765-4d94-afad-6020819aa893___JR_HL 8233.JPG\n",
            "    train/PlantVillage/f536e055-666e-41cc-8ee2-c1af2fbf754a___RS_Early.B 8104.JPG\n",
            "    train/PlantVillage/6173dbe2-1379-4241-a43e-7cbad448b6cc___RS_Early.B 6993.JPG\n",
            "    train/PlantVillage/ffcfa32d-4506-49ed-9e35-972d5cbad0b8___Com.G_SpM_FL 8450.JPG\n",
            "    train/PlantVillage/08f0fd0e-d4b1-400d-9df1-2d61d24c95f0___Com.G_SpM_FL 9397.JPG\n",
            "    train/PlantVillage/7c9ac47e-ec96-457c-96d7-8aefdace978f___Crnl_L.Mold 6850.JPG\n",
            "    train/PlantVillage/7cf516ee-327e-40a6-810b-7c08ea57f825___UF.GRC_YLCV_Lab 02062.JPG\n",
            "    train/PlantVillage/fc6a6a03-757b-4549-8c2e-194141456f60___GCREC_Bact.Sp 6252.JPG\n",
            "    train/PlantVillage/0ce74db6-be9b-4c43-a104-6a3f9bcd2de2___NREC_B.Spot 1827.JPG\n",
            "    train/PlantVillage/9ea77bf9-c078-4adf-9173-ca0dca15dabd___GCREC_Bact.Sp 6333.JPG\n",
            "    train/PlantVillage/71048e39-6d6e-4381-9113-f2b4bd71ab93___Com.G_SpM_FL 8749.JPG\n",
            "    train/PlantVillage/b7fb98ad-68d6-46b9-9f17-9d3f111b20fb___Com.G_SpM_FL 1676.JPG\n",
            "    train/PlantVillage/503c6a57-2af3-48ed-b96f-e2cbf6fb5dd1___PSU_CG 2263.JPG\n",
            "    train/PlantVillage/badeca64-87ed-439a-b373-b4b872dbe173___GCREC_Bact.Sp 6219.JPG\n",
            "    train/PlantVillage/447735c7-b650-4ef6-a53b-8fb661470de3___JR_B.Spot 3370.JPG\n",
            "    train/PlantVillage/58adb74c-a1b3-4b9d-95b7-f0721c499388___RS_LB 4326.JPG\n",
            "    train/PlantVillage/a2664ab8-aca6-4891-9f6f-8cc414d6b231___JR_HL 5849.JPG\n",
            "    train/PlantVillage/b8b7b98a-eb1a-4213-9b0b-aeef4df427e8___RS_HL 1858.JPG\n",
            "    train/PlantVillage/e7619088-871f-422e-a6e8-5c8a7da70b3e___GHLB2 Leaf 8825.JPG\n",
            "\n",
            "== val ==\n",
            "  top-level dirs: ['PlantVillage']\n",
            "  wrapper folder: PlantVillage\n",
            "    FILE eed357a4-620c-42ef-b626-cb3b5026abca___GHLB2 Leaf 8970.JPG\n",
            "    FILE 43dde1e0-8c73-45af-8fbf-d500e450a154___RS_LB 3270.JPG\n",
            "    FILE 6173dbe2-1379-4241-a43e-7cbad448b6cc___RS_Early.B 6993.JPG\n",
            "    FILE 05c29b23-ecdf-41fc-be67-910bb74cd67c___RS_LB 4016.JPG\n",
            "    FILE a2664ab8-aca6-4891-9f6f-8cc414d6b231___JR_HL 5849.JPG\n",
            "    FILE 498e5205-4421-4ec6-80b5-e4f79156b1c5___RS_Erly.B 7362.JPG\n",
            "    FILE de4a4bd4-b611-46fd-8190-cff651b56326___Matt.S_CG 6718.JPG\n",
            "    FILE 369ced5b-20e4-4c91-b54e-cff2c3056be6___YLCV_NREC 2739.JPG\n",
            "    FILE 5360a154-707e-47fe-94cf-dbe155b648bd___GCREC_Bact.Sp 3270.JPG\n",
            "    FILE 26017f82-c32f-4316-8dd9-559a008788e8___UF.GRC_YLCV_Lab 02797.JPG\n",
            "    FILE e148d32e-b3e5-421f-8f30-838fc4792a71___Com.G_TgS_FL 0916.JPG\n",
            "    FILE 4c6a8199-896f-4cbf-8db4-3baa021a9e1c___NREC_B.Spot 9247.JPG\n",
            "    FILE 2492565a-7851-43ab-a1c2-41b3e0117b4a___JR_HL 7701.JPG\n",
            "    FILE 365ab78f-6a75-49ef-acb8-44b8b21fac2c___NREC_B.Spot 9159.JPG\n",
            "    FILE 81b76342-4d13-4f96-9c4d-ccf4865b3171___YLCV_GCREC 2412.JPG\n",
            "    FILE d91538d7-1e25-442d-8963-86fec71a6545___RS_Late.B 6365.JPG\n",
            "    FILE c912cdc9-37c4-4593-a334-4a5d3a02c2e4___GCREC_Bact.Sp 3063.JPG\n",
            "    FILE 63f6ad32-28e1-459f-a8ab-100f52fc5a2c___JR_HL 7878.JPG\n",
            "    FILE 6eddf6dc-415f-4e8d-b6b6-3080370e228b___Com.G_TgS_FL 0659.JPG\n",
            "    FILE 8bd2f1ec-3094-479f-9210-f7fba270922d___GCREC_Bact.Sp 6256.JPG\n",
            "  sample filenames (first 20):\n",
            "    val/PlantVillage/eed357a4-620c-42ef-b626-cb3b5026abca___GHLB2 Leaf 8970.JPG\n",
            "    val/PlantVillage/43dde1e0-8c73-45af-8fbf-d500e450a154___RS_LB 3270.JPG\n",
            "    val/PlantVillage/6173dbe2-1379-4241-a43e-7cbad448b6cc___RS_Early.B 6993.JPG\n",
            "    val/PlantVillage/05c29b23-ecdf-41fc-be67-910bb74cd67c___RS_LB 4016.JPG\n",
            "    val/PlantVillage/a2664ab8-aca6-4891-9f6f-8cc414d6b231___JR_HL 5849.JPG\n",
            "    val/PlantVillage/498e5205-4421-4ec6-80b5-e4f79156b1c5___RS_Erly.B 7362.JPG\n",
            "    val/PlantVillage/de4a4bd4-b611-46fd-8190-cff651b56326___Matt.S_CG 6718.JPG\n",
            "    val/PlantVillage/369ced5b-20e4-4c91-b54e-cff2c3056be6___YLCV_NREC 2739.JPG\n",
            "    val/PlantVillage/5360a154-707e-47fe-94cf-dbe155b648bd___GCREC_Bact.Sp 3270.JPG\n",
            "    val/PlantVillage/26017f82-c32f-4316-8dd9-559a008788e8___UF.GRC_YLCV_Lab 02797.JPG\n",
            "    val/PlantVillage/e148d32e-b3e5-421f-8f30-838fc4792a71___Com.G_TgS_FL 0916.JPG\n",
            "    val/PlantVillage/4c6a8199-896f-4cbf-8db4-3baa021a9e1c___NREC_B.Spot 9247.JPG\n",
            "    val/PlantVillage/2492565a-7851-43ab-a1c2-41b3e0117b4a___JR_HL 7701.JPG\n",
            "    val/PlantVillage/365ab78f-6a75-49ef-acb8-44b8b21fac2c___NREC_B.Spot 9159.JPG\n",
            "    val/PlantVillage/81b76342-4d13-4f96-9c4d-ccf4865b3171___YLCV_GCREC 2412.JPG\n",
            "    val/PlantVillage/d91538d7-1e25-442d-8963-86fec71a6545___RS_Late.B 6365.JPG\n",
            "    val/PlantVillage/c912cdc9-37c4-4593-a334-4a5d3a02c2e4___GCREC_Bact.Sp 3063.JPG\n",
            "    val/PlantVillage/63f6ad32-28e1-459f-a8ab-100f52fc5a2c___JR_HL 7878.JPG\n",
            "    val/PlantVillage/6eddf6dc-415f-4e8d-b6b6-3080370e228b___Com.G_TgS_FL 0659.JPG\n",
            "    val/PlantVillage/8bd2f1ec-3094-479f-9210-f7fba270922d___GCREC_Bact.Sp 6256.JPG\n",
            "\n",
            "== test ==\n",
            "  top-level dirs: ['PlantVillage']\n",
            "  wrapper folder: PlantVillage\n",
            "    FILE 829d0a5e-326c-4ae6-a3c6-c65297cb5d2f___RS_Erly.B 8338.JPG\n",
            "    FILE eed357a4-620c-42ef-b626-cb3b5026abca___GHLB2 Leaf 8970.JPG\n",
            "    FILE 43dde1e0-8c73-45af-8fbf-d500e450a154___RS_LB 3270.JPG\n",
            "    FILE 05c29b23-ecdf-41fc-be67-910bb74cd67c___RS_LB 4016.JPG\n",
            "    FILE 71048e39-6d6e-4381-9113-f2b4bd71ab93___Com.G_SpM_FL 8749.JPG\n",
            "    FILE b7fb98ad-68d6-46b9-9f17-9d3f111b20fb___Com.G_SpM_FL 1676.JPG\n",
            "    FILE 503c6a57-2af3-48ed-b96f-e2cbf6fb5dd1___PSU_CG 2263.JPG\n",
            "    FILE badeca64-87ed-439a-b373-b4b872dbe173___GCREC_Bact.Sp 6219.JPG\n",
            "    FILE b8b7b98a-eb1a-4213-9b0b-aeef4df427e8___RS_HL 1858.JPG\n",
            "    FILE 85908ad9-57a4-402e-b9c3-3907b452d26b___JR_Sept.L.S 8566.JPG\n",
            "    FILE 79e38a3e-8170-44e3-a7ce-e00bbbcf24dc___GH_HL Leaf 460.JPG\n",
            "    FILE b16f9d46-69a2-4774-ab87-50159dc34dcc___GCREC_Bact.Sp 3471.JPG\n",
            "    FILE 435fc789-e79a-477c-a5f3-a2b9a2c3d18e___YLCV_NREC 2661.JPG\n",
            "    FILE eddbea96-abe9-47be-8bda-12149f061e6a___JR_HL 8884.JPG\n",
            "    FILE 12ce2ab1-14c4-4960-81d4-e6ae776510e9___RS_LB 2880.JPG\n",
            "    FILE e148d32e-b3e5-421f-8f30-838fc4792a71___Com.G_TgS_FL 0916.JPG\n",
            "    FILE 377f30e6-bf06-41d6-885e-85bfe5146a9b___NREC_B.Spot 9072.JPG\n",
            "    FILE efb6e5e5-d83c-4634-a9f6-3f2332713497___RS_Early.B 6788.JPG\n",
            "    FILE c42b5e69-2673-42ef-8e14-a785fea3aad6___RS_Early.B 9107.JPG\n",
            "    FILE 2def1cb7-36cf-4aed-a8e5-ad7864721a44___JR_B.Spot 8915.JPG\n",
            "  sample filenames (first 20):\n",
            "    test/PlantVillage/829d0a5e-326c-4ae6-a3c6-c65297cb5d2f___RS_Erly.B 8338.JPG\n",
            "    test/PlantVillage/eed357a4-620c-42ef-b626-cb3b5026abca___GHLB2 Leaf 8970.JPG\n",
            "    test/PlantVillage/43dde1e0-8c73-45af-8fbf-d500e450a154___RS_LB 3270.JPG\n",
            "    test/PlantVillage/05c29b23-ecdf-41fc-be67-910bb74cd67c___RS_LB 4016.JPG\n",
            "    test/PlantVillage/71048e39-6d6e-4381-9113-f2b4bd71ab93___Com.G_SpM_FL 8749.JPG\n",
            "    test/PlantVillage/b7fb98ad-68d6-46b9-9f17-9d3f111b20fb___Com.G_SpM_FL 1676.JPG\n",
            "    test/PlantVillage/503c6a57-2af3-48ed-b96f-e2cbf6fb5dd1___PSU_CG 2263.JPG\n",
            "    test/PlantVillage/badeca64-87ed-439a-b373-b4b872dbe173___GCREC_Bact.Sp 6219.JPG\n",
            "    test/PlantVillage/b8b7b98a-eb1a-4213-9b0b-aeef4df427e8___RS_HL 1858.JPG\n",
            "    test/PlantVillage/85908ad9-57a4-402e-b9c3-3907b452d26b___JR_Sept.L.S 8566.JPG\n",
            "    test/PlantVillage/79e38a3e-8170-44e3-a7ce-e00bbbcf24dc___GH_HL Leaf 460.JPG\n",
            "    test/PlantVillage/b16f9d46-69a2-4774-ab87-50159dc34dcc___GCREC_Bact.Sp 3471.JPG\n",
            "    test/PlantVillage/435fc789-e79a-477c-a5f3-a2b9a2c3d18e___YLCV_NREC 2661.JPG\n",
            "    test/PlantVillage/eddbea96-abe9-47be-8bda-12149f061e6a___JR_HL 8884.JPG\n",
            "    test/PlantVillage/12ce2ab1-14c4-4960-81d4-e6ae776510e9___RS_LB 2880.JPG\n",
            "    test/PlantVillage/e148d32e-b3e5-421f-8f30-838fc4792a71___Com.G_TgS_FL 0916.JPG\n",
            "    test/PlantVillage/377f30e6-bf06-41d6-885e-85bfe5146a9b___NREC_B.Spot 9072.JPG\n",
            "    test/PlantVillage/efb6e5e5-d83c-4634-a9f6-3f2332713497___RS_Early.B 6788.JPG\n",
            "    test/PlantVillage/c42b5e69-2673-42ef-8e14-a785fea3aad6___RS_Early.B 9107.JPG\n",
            "    test/PlantVillage/2def1cb7-36cf-4aed-a8e5-ad7864721a44___JR_B.Spot 8915.JPG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Auto-fix dataset structure: flatten wrapper OR create class folders from filenames\n",
        "import os, shutil, re\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "BASE = Path('/content/plant_disease')\n",
        "IMG_EXTS = {'.jpg','.jpeg','.png','.bmp','.tif','.tiff'}\n",
        "\n",
        "def list_dirs(path):\n",
        "    return sorted([p for p in Path(path).iterdir() if p.is_dir()])\n",
        "\n",
        "def contains_class_subfolders(wrapper):\n",
        "    subs = [d for d in wrapper.iterdir() if d.is_dir()]\n",
        "    # if many subdirs and they contain images -> it's class-root\n",
        "    return any(any(f.suffix.lower() in IMG_EXTS for f in sub.rglob('*') if f.is_file()) for sub in subs)\n",
        "\n",
        "# Try two strategies for each split\n",
        "for split in ['train','val','test']:\n",
        "    split_dir = BASE / split\n",
        "    if not split_dir.exists():\n",
        "        print(\"Skip (missing):\", split_dir); continue\n",
        "    top_dirs = list_dirs(split_dir)\n",
        "    print(\"\\nProcessing split:\", split, \"top_dirs:\", [d.name for d in top_dirs][:10])\n",
        "    # If there is a single wrapper folder (like PlantVillage) and inside it are class folders -> move them up\n",
        "    if len(top_dirs) == 1 and contains_class_subfolders(top_dirs[0]):\n",
        "        wrapper = top_dirs[0]\n",
        "        print(\"  Found wrapper with class subfolders:\", wrapper)\n",
        "        for sub in sorted([d for d in wrapper.iterdir() if d.is_dir()]):\n",
        "            dest = split_dir / sub.name\n",
        "            if dest.exists():\n",
        "                print(f\"   dest exists, skipping move: {dest}\")\n",
        "            else:\n",
        "                shutil.move(str(sub), str(dest))\n",
        "                print(f\"   moved {sub.name} -> {dest}\")\n",
        "        # attempt to remove wrapper if empty (leave files if any)\n",
        "        try:\n",
        "            if not any(wrapper.iterdir()):\n",
        "                wrapper.rmdir()\n",
        "                print(\"   removed empty wrapper\", wrapper)\n",
        "            else:\n",
        "                print(\"   wrapper still contains files; not removed.\")\n",
        "        except Exception as e:\n",
        "            print(\"   could not remove wrapper:\", e)\n",
        "    else:\n",
        "        # If there are no class subfolders (only files), try parse labels from filenames\n",
        "        # Collect files directly under split_dir (or deeper first level)\n",
        "        file_list = [f for f in split_dir.rglob('*') if f.is_file() and f.suffix.lower() in IMG_EXTS]\n",
        "        # Heuristic: many filenames include '___' (PlantVillage style)\n",
        "        pattern_counts = Counter()\n",
        "        sample = file_list[:200]\n",
        "        for f in sample:\n",
        "            name = f.name\n",
        "            if '___' in name:\n",
        "                label = name.split('___')[0]\n",
        "                pattern_counts['triple_underscore'] += 1\n",
        "            elif '_' in name:\n",
        "                # maybe label before first underscore\n",
        "                label = name.split('_')[0]\n",
        "                pattern_counts['single_underscore'] += 1\n",
        "            else:\n",
        "                pattern_counts['no_sep'] += 1\n",
        "        print(\"  sample pattern counts:\", pattern_counts)\n",
        "        # If triple_underscore majority, build class folders from that\n",
        "        if pattern_counts['triple_underscore'] > max(pattern_counts['single_underscore'], pattern_counts['no_sep']):\n",
        "            print(\"  Using '___' split from filename to create class folders.\")\n",
        "            for f in file_list:\n",
        "                parts = f.name.split('___')\n",
        "                if len(parts) >= 2:\n",
        "                    cls = parts[0]\n",
        "                else:\n",
        "                    cls = 'unknown'\n",
        "                dest_dir = split_dir / cls\n",
        "                dest_dir.mkdir(exist_ok=True)\n",
        "                shutil.copy2(str(f), str(dest_dir / f.name))\n",
        "        elif pattern_counts['single_underscore'] > pattern_counts['no_sep']:\n",
        "            print(\"  Using '_' split from filename to create class folders (prefix before '_' will be class).\")\n",
        "            for f in file_list:\n",
        "                cls = f.name.split('_')[0]\n",
        "                dest_dir = split_dir / cls\n",
        "                dest_dir.mkdir(exist_ok=True)\n",
        "                shutil.copy2(str(f), str(dest_dir / f.name))\n",
        "        else:\n",
        "            print(\"  No clear filename pattern detected; please examine sample filenames manually.\")\n",
        "            print(\"  Sample files:\")\n",
        "            for s in sample[:30]:\n",
        "                print(\"   \", s.name)\n",
        "            # do not attempt destructive moves\n",
        "            continue\n",
        "\n",
        "# After organizing, print summary counts\n",
        "print(\"\\nFinal per-split summary:\")\n",
        "for split in ['train','val','test']:\n",
        "    sdir = BASE / split\n",
        "    if not sdir.exists():\n",
        "        print(split, \"missing\")\n",
        "        continue\n",
        "    classes = [d for d in sdir.iterdir() if d.is_dir()]\n",
        "    print(f\"\\n{split}: {len(classes)} classes\")\n",
        "    for c in classes[:40]:\n",
        "        cnt = len([f for f in c.rglob('*') if f.is_file() and f.suffix.lower() in IMG_EXTS])\n",
        "        print(\" \", c.name, cnt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCL6eAXse3CF",
        "outputId": "e9e8d479-101f-43ef-f292-30e8b7bdac14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing split: train top_dirs: ['PlantVillage']\n",
            "  sample pattern counts: Counter({'triple_underscore': 200})\n",
            "  Using '___' split from filename to create class folders.\n",
            "\n",
            "Processing split: val top_dirs: ['PlantVillage']\n",
            "  sample pattern counts: Counter({'triple_underscore': 200})\n",
            "  Using '___' split from filename to create class folders.\n",
            "\n",
            "Processing split: test top_dirs: ['PlantVillage']\n",
            "  sample pattern counts: Counter({'triple_underscore': 200})\n",
            "  Using '___' split from filename to create class folders.\n",
            "\n",
            "Final per-split summary:\n",
            "\n",
            "train: 18786 classes\n",
            "  89cff955-c142-4874-aa13-a0b9ca6c11e1 1\n",
            "  d648ce42-6742-44b2-95b9-a01b24e3054a 1\n",
            "  5f96c793-8015-458b-b3fe-7e718b673b1b 1\n",
            "  e0d2d9f6-b29e-4cd8-8c2b-7d462271ceb3 1\n",
            "  a7c1d137-6609-4659-94ce-81c62f64665e 1\n",
            "  cec3cfff-3d00-46cf-8a8e-b7438ec7901e 1\n",
            "  ac205c4a-58c5-430f-8b4c-f853652df749 1\n",
            "  04c8e6b9-7710-4cdd-b259-2d78b15d1036 1\n",
            "  707da75e-4924-4033-b141-e8a391f39741 1\n",
            "  58d19fbc-49ee-4738-b0e0-88afd097cefb 1\n",
            "  072c957f-2ed9-4026-a36c-7d95b289e44a 1\n",
            "  411f8908-2bb4-45dc-b1d1-314cc951f0a9 1\n",
            "  25bb0739-cbd4-4e75-a6f4-67ca5cc7f986 1\n",
            "  452dd2b2-6d8e-46d5-a791-6e693fd9354b 1\n",
            "  0d8445ac-6333-42b0-bf32-950a2dd83908 1\n",
            "  6423c12d-7802-4fd0-92d3-2f5438f22418 1\n",
            "  dbd289fc-1399-44fe-968d-39a5a4f16679 1\n",
            "  bcc50c85-c7d1-4da7-a121-e15d028aef69 1\n",
            "  5507fb57-ee99-4dfc-8955-c5ec00e4d4d8 1\n",
            "  e896d63c-c7e2-48ef-bb32-000a35655e9a 1\n",
            "  036d12b0-1e97-4975-bd80-a8a3c48588e7 1\n",
            "  4a1c0d21-3ba1-4c62-9e6e-f12b845d54c9 1\n",
            "  4a1ba341-501b-4f74-879b-61a9e29a8f9a 1\n",
            "  a5b278e7-448e-40ae-80c2-d286f2681d10 1\n",
            "  46e93f7b-651f-4fbf-afad-d5a4312bc50f 1\n",
            "  731d3112-8dfa-4ce0-b27e-6a0cdb661a6d 1\n",
            "  31f5ede8-b8de-44db-8f0e-11275028104c 1\n",
            "  d674cbf6-0cdc-407a-97ea-c3f53f2c51e4 1\n",
            "  3531f15b-53f7-4e0f-b80e-64a7b0e26fc9 1\n",
            "  3682433d-fb0a-495b-a6e8-d753542b042b 1\n",
            "  be3326b5-c152-464d-8803-21ca7149e6db 1\n",
            "  a601d086-0999-4318-ac04-a4bbc7b64e5e 1\n",
            "  9f2c6d36-700b-4eec-9478-31cfaaed166e 1\n",
            "  c86292c5-5f49-498d-9339-5f8d5d0b7492 1\n",
            "  5deee0ba-6bbb-450e-a107-102063e2f93b 1\n",
            "  2d7fa670-2828-440e-aba2-1f7a843988ea 1\n",
            "  f6202c30-deb0-498d-8e49-44b012204547 1\n",
            "  9fc9ea86-e97f-4b96-8838-6e614196b2f6 1\n",
            "  6a3ae349-27f8-468c-a1ad-7ddfc31e3435 1\n",
            "  c488f861-d42c-4527-846d-1e352fbd72a3 1\n",
            "\n",
            "val: 5735 classes\n",
            "  760d6620-5a8a-4566-92bf-8a3b5a8d81b2 1\n",
            "  e0d2d9f6-b29e-4cd8-8c2b-7d462271ceb3 1\n",
            "  072c957f-2ed9-4026-a36c-7d95b289e44a 1\n",
            "  bcc50c85-c7d1-4da7-a121-e15d028aef69 1\n",
            "  5507fb57-ee99-4dfc-8955-c5ec00e4d4d8 1\n",
            "  a5b278e7-448e-40ae-80c2-d286f2681d10 1\n",
            "  31f5ede8-b8de-44db-8f0e-11275028104c 1\n",
            "  d674cbf6-0cdc-407a-97ea-c3f53f2c51e4 1\n",
            "  3682433d-fb0a-495b-a6e8-d753542b042b 1\n",
            "  c86292c5-5f49-498d-9339-5f8d5d0b7492 1\n",
            "  12342367-f62b-40d0-897b-77bd1edbd3d1 1\n",
            "  c3cd2ad3-174e-48b4-a7a5-ce63be745fd5 1\n",
            "  94255d4c-a15a-4a7d-8f20-f61c69d08242 1\n",
            "  08945f1c-9204-48d6-a5b4-bece52a18909 1\n",
            "  8a77734b-9b2f-44d7-a7cb-2f32b15bd9fe 1\n",
            "  5a3f63fd-717e-442a-8fa4-a904ec2dfe2f 1\n",
            "  4e33d562-0e40-45ca-adb1-4a7fb4999078 1\n",
            "  2accd1d7-e6b4-4114-9f8b-11130d583320 1\n",
            "  09f556e6-ee96-4397-90c7-e491169ad065 1\n",
            "  162156e8-724a-48d9-8a19-904937f9dc03 1\n",
            "  3cb86ee4-88dd-45e0-99cc-6dfed27919bf 1\n",
            "  1321c4b9-86db-41d1-a93b-b036040cc977 1\n",
            "  837c2369-4f38-4c58-b1ed-fa83f96258ad 1\n",
            "  d0dceae1-6bf8-475a-99fa-f27accadece0 1\n",
            "  89e3fc22-17bc-469b-8443-ffb4ed9a0c2f 1\n",
            "  eea69188-8c51-4d11-9e2d-4aec3e91a078 1\n",
            "  f3ba91af-7c1d-48a6-806b-1ea9235dc39d 1\n",
            "  5f67b4d4-4503-4149-950b-6cd08a20aaf8 1\n",
            "  8b1d733e-6b10-4fcd-83aa-7385c56b2a14 1\n",
            "  e027b3a9-1975-4a95-bc2b-10d361dd7ca2 1\n",
            "  7f861a7a-5d4b-40e7-8d5f-0df63c08706c 1\n",
            "  2433614e-78d3-45ae-b719-59efb0397572 1\n",
            "  65b2d5c2-a737-43cf-b83f-dfa749c1d143 1\n",
            "  cc799066-155d-4d7d-9229-93db4e6be216 1\n",
            "  2a7e1698-9747-40e6-994e-5d4dc1d2b239 1\n",
            "  dd2bf52d-685d-4feb-ac7e-846cbd29d9c3 1\n",
            "  02fd4e60-db82-441c-87f6-08de768a4462 1\n",
            "  8ac16bec-8484-499f-a86d-fbc6bcbe1c29 1\n",
            "  5dd2a498-7eff-4da7-9c0f-8c394103e52c 1\n",
            "  e6d642b7-1313-4b97-b58b-eab8a92cae64 1\n",
            "\n",
            "test: 5759 classes\n",
            "  760d6620-5a8a-4566-92bf-8a3b5a8d81b2 1\n",
            "  a7c1d137-6609-4659-94ce-81c62f64665e 1\n",
            "  ac205c4a-58c5-430f-8b4c-f853652df749 1\n",
            "  411f8908-2bb4-45dc-b1d1-314cc951f0a9 1\n",
            "  25bb0739-cbd4-4e75-a6f4-67ca5cc7f986 1\n",
            "  452dd2b2-6d8e-46d5-a791-6e693fd9354b 1\n",
            "  0d8445ac-6333-42b0-bf32-950a2dd83908 1\n",
            "  e896d63c-c7e2-48ef-bb32-000a35655e9a 1\n",
            "  4a1ba341-501b-4f74-879b-61a9e29a8f9a 1\n",
            "  731d3112-8dfa-4ce0-b27e-6a0cdb661a6d 1\n",
            "  3531f15b-53f7-4e0f-b80e-64a7b0e26fc9 1\n",
            "  be3326b5-c152-464d-8803-21ca7149e6db 1\n",
            "  a601d086-0999-4318-ac04-a4bbc7b64e5e 1\n",
            "  5deee0ba-6bbb-450e-a107-102063e2f93b 1\n",
            "  2d7fa670-2828-440e-aba2-1f7a843988ea 1\n",
            "  9fc9ea86-e97f-4b96-8838-6e614196b2f6 1\n",
            "  6a3ae349-27f8-468c-a1ad-7ddfc31e3435 1\n",
            "  3f798251-1d4b-4d93-968f-18243a9df007 1\n",
            "  12342367-f62b-40d0-897b-77bd1edbd3d1 1\n",
            "  84f666da-9e17-4e90-9813-0ed64da175d9 1\n",
            "  344a9b3a-0748-4fb0-9622-83be2ebfccf1 1\n",
            "  5603219d-7494-4cf2-98f2-60d7fd5e62a4 1\n",
            "  c7da5378-82bb-45ef-9b9c-2c64c4130cf6 1\n",
            "  08945f1c-9204-48d6-a5b4-bece52a18909 1\n",
            "  39b1d9c6-7a13-426f-9154-563e6a8d0438 1\n",
            "  8a77734b-9b2f-44d7-a7cb-2f32b15bd9fe 1\n",
            "  fb0f6e2b-1f21-406f-83d3-2c6236d47ca0 1\n",
            "  735de814-ec86-4a2e-b523-e5d5542aff2b 1\n",
            "  147582b1-0d36-4409-8810-1fd5d4304388 1\n",
            "  851ea0ae-63c2-4f43-825e-e5e60b636ec0 1\n",
            "  09f556e6-ee96-4397-90c7-e491169ad065 1\n",
            "  0a334ae6-bea3-4453-b200-85e082794d56 1\n",
            "  c9ff37ca-a310-49ad-8978-0913b3c55f58 1\n",
            "  b8b4064f-208a-46f9-b13e-5096fdbdb429 1\n",
            "  2bdca297-e0d1-41f0-b1dc-3801384faa1a 1\n",
            "  1e09bb35-ef0d-4efb-b3eb-320a976a5e13 1\n",
            "  7f861a7a-5d4b-40e7-8d5f-0df63c08706c 1\n",
            "  84c43d92-9e49-4ce6-b094-5df767b2b0df 1\n",
            "  df53547e-28ef-4619-8b01-aa2d50e6c8b3 1\n",
            "  2ce6e1ab-1743-44f9-b3c2-d70304eb9366 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell A — build fixed dataset by parsing label after '___' in filenames\n",
        "import os, shutil, re\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "SRC_BASE = Path('/content/plant_disease')   # current unzipped folder\n",
        "FIXED_BASE = Path('/content/plant_disease_fixed')  # new clean dataset location\n",
        "IMG_EXTS = {'.jpg','.jpeg','.png','.bmp','.tif','.tiff'}\n",
        "\n",
        "print(\"SRC_BASE exists:\", SRC_BASE.exists())\n",
        "# remove old fixed dir if present (optional) — comment out if you want to keep previous run\n",
        "if FIXED_BASE.exists():\n",
        "    print(\"Removing old fixed dir:\", FIXED_BASE)\n",
        "    shutil.rmtree(FIXED_BASE)\n",
        "FIXED_BASE.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def sanitize_label(s):\n",
        "    # keep letters, numbers, underscore, hyphen, dot; replace other chars with underscore\n",
        "    s = s.strip()\n",
        "    s = re.sub(r'\\s+', '_', s)                 # spaces -> underscore\n",
        "    s = re.sub(r'[^\\w\\-\\.\\u00C0-\\u017F]', '_', s)  # allow unicode letters too\n",
        "    return s\n",
        "\n",
        "for split in ['train','val','test']:\n",
        "    src_split = SRC_BASE / split\n",
        "    dest_split = FIXED_BASE / split\n",
        "    dest_split.mkdir(parents=True, exist_ok=True)\n",
        "    if not src_split.exists():\n",
        "        print(\"  WARNING: source split missing:\", src_split)\n",
        "        continue\n",
        "\n",
        "    # collect all image files under this split (search recursively)\n",
        "    all_files = [p for p in src_split.rglob('*') if p.is_file() and p.suffix.lower() in IMG_EXTS]\n",
        "    print(f\"\\nProcessing split={split}, found {len(all_files)} image files\")\n",
        "\n",
        "    created = Counter()\n",
        "    for f in all_files:\n",
        "        name = f.name\n",
        "        # primary case: '...___LABEL <number>.ext' (PlantVillage)\n",
        "        if '___' in name:\n",
        "            after = name.split('___',1)[1]                # e.g. \"RS_Erly.B 8338.JPG\"\n",
        "            # remove extension\n",
        "            after_noext = os.path.splitext(after)[0]     # \"RS_Erly.B 8338\"\n",
        "            # remove trailing numeric id tokens (e.g. \" 8338\")\n",
        "            after_noid = re.sub(r'\\s+\\d+$', '', after_noext)\n",
        "            label_raw = after_noid.strip()\n",
        "            label = sanitize_label(label_raw)\n",
        "        else:\n",
        "            # fallback: try prefix before first underscore or put into unknown\n",
        "            if '_' in name:\n",
        "                label = sanitize_label(name.split('_')[0])\n",
        "            else:\n",
        "                label = 'unknown'\n",
        "        # create destination and copy\n",
        "        dest_dir = dest_split / label\n",
        "        dest_dir.mkdir(parents=True, exist_ok=True)\n",
        "        dest_path = dest_dir / name\n",
        "        # if file already exists (duplicate names), add suffix\n",
        "        if dest_path.exists():\n",
        "            base, ext = os.path.splitext(name)\n",
        "            i = 1\n",
        "            while True:\n",
        "                newname = f\"{base}_{i}{ext}\"\n",
        "                if not (dest_dir / newname).exists():\n",
        "                    dest_path = dest_dir / newname\n",
        "                    break\n",
        "                i += 1\n",
        "        shutil.copy2(str(f), str(dest_path))\n",
        "        created[label] += 1\n",
        "\n",
        "    print(f\"  Created {len(created)} class folders for split {split}. Sample classes (first 40):\")\n",
        "    for cls, cnt in list(created.items())[:40]:\n",
        "        print(\"   \", cls, cnt)\n",
        "\n",
        "print(\"\\nDone. Fixed dataset created at:\", FIXED_BASE)\n",
        "print(\"You may now run the image_dataset_from_directory cell pointed at /content/plant_disease_fixed\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5HpEP-Ae3F-",
        "outputId": "ca5075a7-a9b1-4c18-fc45-92f9b0b32779"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SRC_BASE exists: True\n",
            "\n",
            "Processing split=train, found 37570 image files\n",
            "  Created 541 class folders for split train. Sample classes (first 40):\n",
            "    YLCV_NREC 1376\n",
            "    RS_Late.B 1654\n",
            "    RS_Erly.B 1840\n",
            "    RS_HL 2140\n",
            "    JR_HL 2670\n",
            "    GHLB_PS_Leaf_30_Day 4\n",
            "    RS_Early.B 1846\n",
            "    Com.G_SpM_FL 3088\n",
            "    UF.GRC_YLCV_Lab 2888\n",
            "    GHLB2_Leaf 1090\n",
            "    NREC_B.Spot 748\n",
            "    UF.GRC_BS_Lab_Leaf 674\n",
            "    YLCV_GCREC 1566\n",
            "    Com.G_TgS_FL 2536\n",
            "    Crnl_L.Mold 1752\n",
            "    GH_HL_Leaf 574\n",
            "    GCREC_Bact.Sp 3156\n",
            "    JR_Sept.L.S 914\n",
            "    RS_LB 1796\n",
            "    JR_B.Spot 1066\n",
            "    GH_HL_Leaf_310.1 2\n",
            "    Keller.St_CG 518\n",
            "    Matt.S_CG 1836\n",
            "    PSU_CG 660\n",
            "    GHLB_PS_Leaf_1_Day 10\n",
            "    GH_HL_Leaf_495.1 2\n",
            "    GH_HL_Leaf_226.6 2\n",
            "    GHLB_Leaf_2.1_Day 4\n",
            "    GHLB_PS_Leaf_17.1_Day 4\n",
            "    GHLB_Leaf_10_Day 2\n",
            "    GH_HL_Leaf_469.1 2\n",
            "    GH_HL_Leaf_466.1 4\n",
            "    GHLB2ES_Leaf_62.1 2\n",
            "    GH_HL_Leaf_264.1 2\n",
            "    GH_HL_Leaf_317.1 2\n",
            "    GHLB_PS_Leaf_23.7_Day 4\n",
            "    GHLB_PS_Leaf_35.1_Day 2\n",
            "    GHLB2_Leaf_125.2 2\n",
            "    GHLB2_Leaf_154.4 2\n",
            "    GH_HL_Leaf_237.1 2\n",
            "\n",
            "Processing split=val, found 11468 image files\n",
            "  Created 198 class folders for split val. Sample classes (first 40):\n",
            "    GCREC_Bact.Sp 994\n",
            "    RS_HL 606\n",
            "    UF.GRC_YLCV_Lab 892\n",
            "    UF.GRC_BS_Lab_Leaf 200\n",
            "    YLCV_GCREC 526\n",
            "    RS_Early.B 550\n",
            "    GH_HL_Leaf 184\n",
            "    JR_HL 836\n",
            "    JR_B.Spot 384\n",
            "    RS_LB 514\n",
            "    YLCV_NREC 414\n",
            "    Com.G_SpM_FL 870\n",
            "    GHLB2_Leaf 374\n",
            "    Com.G_TgS_FL 780\n",
            "    JR_Sept.L.S 258\n",
            "    RS_Erly.B 508\n",
            "    Matt.S_CG 530\n",
            "    NREC_B.Spot 228\n",
            "    PSU_CG 240\n",
            "    RS_Late.B 506\n",
            "    GHLB_Leaf_2.1_Day 4\n",
            "    Crnl_L.Mold 536\n",
            "    Keller.St_CG 158\n",
            "    GHLB_PS_Leaf_23.7_Day 2\n",
            "    GHLB2_Leaf_125.2 2\n",
            "    GHLB2_Leaf_154.4 2\n",
            "    GH_HL_Leaf_196.1 2\n",
            "    GH_HL_Leaf_372.2 2\n",
            "    GHLB2ES_Leaf 10\n",
            "    GH_HL_Leaf_173.5 2\n",
            "    GHLB2_Leaf_80.1 2\n",
            "    GHLB2_Leaf_110.1 2\n",
            "    GH_HL_Leaf_362.1 2\n",
            "    GHLB2_Leaf_106.1 2\n",
            "    GH_HL_Leaf_523.1 2\n",
            "    GHLB2_Leaf_148.4 2\n",
            "    GH_HL_Leaf_434.2 2\n",
            "    GHLB2_Leaf_159.1 2\n",
            "    GHLB_PS_Leaf_49.1_Day 2\n",
            "    GHLB_PS_Leaf_1_Day 4\n",
            "\n",
            "Processing split=test, found 11516 image files\n",
            "  Created 181 class folders for split test. Sample classes (first 40):\n",
            "    GCREC_Bact.Sp 1002\n",
            "    RS_HL 598\n",
            "    GHLB_PS_Leaf_30_Day 4\n",
            "    RS_Erly.B 556\n",
            "    GHLB2_Leaf 340\n",
            "    JR_HL 864\n",
            "    NREC_B.Spot 212\n",
            "    Com.G_TgS_FL 806\n",
            "    Com.G_SpM_FL 864\n",
            "    RS_Early.B 564\n",
            "    JR_Sept.L.S 280\n",
            "    YLCV_GCREC 516\n",
            "    JR_B.Spot 336\n",
            "    RS_LB 652\n",
            "    YLCV_NREC 412\n",
            "    UF.GRC_YLCV_Lab 792\n",
            "    GH_HL_Leaf_310.1 2\n",
            "    Keller.St_CG 148\n",
            "    RS_Late.B 526\n",
            "    Matt.S_CG 590\n",
            "    PSU_CG 210\n",
            "    Crnl_L.Mold 530\n",
            "    GH_HL_Leaf_226.6 2\n",
            "    UF.GRC_BS_Lab_Leaf 202\n",
            "    GH_HL_Leaf 180\n",
            "    GH_HL_Leaf_237.1 2\n",
            "    GH_HL_Leaf_205.1 2\n",
            "    GHLB_Leaf_23.3_Day 2\n",
            "    GHLB_PS_Leaf_28.2_Day 2\n",
            "    GHLB2_Leaf_100.1 2\n",
            "    GHLB_Leaf_1.2_Day 4\n",
            "    GHLB2_Leaf_110.1 2\n",
            "    GHLB_Leaf_8_Day 2\n",
            "    GHLB_PS_Leaf_13.1_Day 2\n",
            "    GHLB_PS_Leaf_17.1_Day 2\n",
            "    GH_HL_Leaf_317.2 2\n",
            "    GHLB_Leaf_2_Day 2\n",
            "    GH_HL_Leaf_502.4 2\n",
            "    bell-pepper-plant-61726 2\n",
            "    GHLB_PS_Leaf_1_Day 6\n",
            "\n",
            "Done. Fixed dataset created at: /content/plant_disease_fixed\n",
            "You may now run the image_dataset_from_directory cell pointed at /content/plant_disease_fixed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell B — create datasets from fixed path and show detected classes\n",
        "import tensorflow as tf, json, os\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "DATA_DIR = '/content/plant_disease_fixed'\n",
        "IMG_SIZE = (224,224)\n",
        "BATCH_SIZE = 32   # lower to 16/8 if OOM\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_dir = os.path.join(DATA_DIR, 'train')\n",
        "val_dir = os.path.join(DATA_DIR, 'val')\n",
        "test_dir = os.path.join(DATA_DIR, 'test')\n",
        "\n",
        "print(\"Checking folders exist:\", os.path.exists(train_dir), os.path.exists(val_dir), os.path.exists(test_dir))\n",
        "\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    train_dir, labels='inferred', label_mode='categorical',\n",
        "    image_size=IMG_SIZE, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    val_dir, labels='inferred', label_mode='categorical',\n",
        "    image_size=IMG_SIZE, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    test_dir, labels='inferred', label_mode='categorical',\n",
        "    image_size=IMG_SIZE, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "class_names = train_ds.class_names\n",
        "print(\"Detected classes:\", len(class_names))\n",
        "print(\"Sample class names (first 50):\", class_names[:50])\n",
        "\n",
        "# save classes.json to Drive models folder (if needed)\n",
        "MODELS_DIR = '/content/drive/MyDrive/Buildable-ML-DL-Fellowship/models'\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "with open(os.path.join(MODELS_DIR,'classes.json'),'w') as f:\n",
        "    json.dump(class_names, f)\n",
        "\n",
        "# Prefetch\n",
        "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "# print number of batches\n",
        "try:\n",
        "    print(\"Train batches:\", tf.data.experimental.cardinality(train_ds).numpy())\n",
        "except Exception:\n",
        "    pass\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpeFVH_Ve3Jq",
        "outputId": "275f8193-b69c-4388-80f0-9117d60e450e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking folders exist: True True True\n",
            "Found 37570 files belonging to 541 classes.\n",
            "Found 11468 files belonging to 198 classes.\n",
            "Found 11516 files belonging to 181 classes.\n",
            "Detected classes: 541\n",
            "Sample class names (first 50): ['2700323949_95aa2eaa01_o', 'CG1', 'Com.G_SpM_FL', 'Com.G_TgS_FL', 'Crnl_L.Mold', 'GCREC_Bact.Sp', 'GHLB2ES_Leaf', 'GHLB2ES_Leaf_119.1', 'GHLB2ES_Leaf_119.2', 'GHLB2ES_Leaf_136.1', 'GHLB2ES_Leaf_138.1', 'GHLB2ES_Leaf_139.1', 'GHLB2ES_Leaf_141.1', 'GHLB2ES_Leaf_62.1', 'GHLB2ES_Leaf_63.1', 'GHLB2ES_Leaf_65.1', 'GHLB2ES_Leaf_66.1', 'GHLB2ES_Leaf_69.1', 'GHLB2ES_Leaf_69.3', 'GHLB2_Leaf', 'GHLB2_Leaf_100.1', 'GHLB2_Leaf_101.1', 'GHLB2_Leaf_101.2', 'GHLB2_Leaf_101.3', 'GHLB2_Leaf_105.1', 'GHLB2_Leaf_106.1', 'GHLB2_Leaf_107.1', 'GHLB2_Leaf_107.3', 'GHLB2_Leaf_108.1', 'GHLB2_Leaf_109.1', 'GHLB2_Leaf_111.1', 'GHLB2_Leaf_111.2', 'GHLB2_Leaf_112.1', 'GHLB2_Leaf_113.2', 'GHLB2_Leaf_113.3', 'GHLB2_Leaf_113.4', 'GHLB2_Leaf_114.1', 'GHLB2_Leaf_114.3', 'GHLB2_Leaf_115.1', 'GHLB2_Leaf_115.2', 'GHLB2_Leaf_116.1', 'GHLB2_Leaf_116.2', 'GHLB2_Leaf_117.1', 'GHLB2_Leaf_117.2', 'GHLB2_Leaf_117.3', 'GHLB2_Leaf_117.4', 'GHLB2_Leaf_118.1', 'GHLB2_Leaf_120.1', 'GHLB2_Leaf_121.1', 'GHLB2_Leaf_121.2']\n",
            "Train batches: 1175\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1 — recreate tf.data from fixed dataset\n",
        "import tensorflow as tf, json, os\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_DIR = '/content/plant_disease_fixed'   # <- fixed dataset\n",
        "IMG_SIZE = (224,224)\n",
        "BATCH_SIZE = 32   # if OOM, set to 16 or 8\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_dir = os.path.join(DATA_DIR, 'train')\n",
        "val_dir = os.path.join(DATA_DIR, 'val')\n",
        "test_dir = os.path.join(DATA_DIR, 'test')\n",
        "\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    train_dir, labels='inferred', label_mode='categorical',\n",
        "    image_size=IMG_SIZE, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    val_dir, labels='inferred', label_mode='categorical',\n",
        "    image_size=IMG_SIZE, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    test_dir, labels='inferred', label_mode='categorical',\n",
        "    image_size=IMG_SIZE, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "class_names = train_ds.class_names\n",
        "num_classes = len(class_names)\n",
        "print(\"Detected classes:\", num_classes)\n",
        "print(\"Sample classes (first 40):\", class_names[:40])\n",
        "\n",
        "# save classes mapping to Drive\n",
        "MODELS_DIR = '/content/drive/MyDrive/Buildable-ML-DL-Fellowship/models'\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "with open(os.path.join(MODELS_DIR,'classes.json'),'w') as f:\n",
        "    json.dump(class_names, f)\n",
        "\n",
        "# prefetch\n",
        "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.prefetch(buffer_size=AUTOTUNE)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzJnbHDXSE2u",
        "outputId": "e0379049-142c-4fbc-bfae-bab0a31e5956"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 37570 files belonging to 541 classes.\n",
            "Found 11468 files belonging to 198 classes.\n",
            "Found 11516 files belonging to 181 classes.\n",
            "Detected classes: 541\n",
            "Sample classes (first 40): ['2700323949_95aa2eaa01_o', 'CG1', 'Com.G_SpM_FL', 'Com.G_TgS_FL', 'Crnl_L.Mold', 'GCREC_Bact.Sp', 'GHLB2ES_Leaf', 'GHLB2ES_Leaf_119.1', 'GHLB2ES_Leaf_119.2', 'GHLB2ES_Leaf_136.1', 'GHLB2ES_Leaf_138.1', 'GHLB2ES_Leaf_139.1', 'GHLB2ES_Leaf_141.1', 'GHLB2ES_Leaf_62.1', 'GHLB2ES_Leaf_63.1', 'GHLB2ES_Leaf_65.1', 'GHLB2ES_Leaf_66.1', 'GHLB2ES_Leaf_69.1', 'GHLB2ES_Leaf_69.3', 'GHLB2_Leaf', 'GHLB2_Leaf_100.1', 'GHLB2_Leaf_101.1', 'GHLB2_Leaf_101.2', 'GHLB2_Leaf_101.3', 'GHLB2_Leaf_105.1', 'GHLB2_Leaf_106.1', 'GHLB2_Leaf_107.1', 'GHLB2_Leaf_107.3', 'GHLB2_Leaf_108.1', 'GHLB2_Leaf_109.1', 'GHLB2_Leaf_111.1', 'GHLB2_Leaf_111.2', 'GHLB2_Leaf_112.1', 'GHLB2_Leaf_113.2', 'GHLB2_Leaf_113.3', 'GHLB2_Leaf_113.4', 'GHLB2_Leaf_114.1', 'GHLB2_Leaf_114.3', 'GHLB2_Leaf_115.1', 'GHLB2_Leaf_115.2']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2 — inspect class counts and (optionally) filter small classes\n",
        "from collections import Counter\n",
        "import glob\n",
        "\n",
        "MIN_SAMPLES = 20   # adjust: keep classes with at least this many images total (train+val+test)\n",
        "BASE = Path('/content/plant_disease_fixed')\n",
        "classes = sorted([d.name for d in (BASE/'train').iterdir() if d.is_dir()])\n",
        "\n",
        "counts = {}\n",
        "for c in classes:\n",
        "    total = sum(len(list((BASE/s).glob(f'{c}/*'))) for s in ['train','val','test'])\n",
        "    counts[c] = total\n",
        "\n",
        "# print summary\n",
        "sorted_counts = sorted(counts.items(), key=lambda x: -x[1])\n",
        "print(\"Top 40 classes by total image count:\")\n",
        "for name, cnt in sorted_counts[:40]:\n",
        "    print(\" \", name, cnt)\n",
        "print(\"\\nClasses with < {} images: {}\".format(MIN_SAMPLES, sum(1 for _,v in counts.items() if v < MIN_SAMPLES)))\n",
        "\n",
        "# If you want to keep only classes >= MIN_SAMPLES, set FILTER=True\n",
        "FILTER = False   # set to True if you want to prune small classes\n",
        "if FILTER:\n",
        "    keep = [name for name,c in counts.items() if c >= MIN_SAMPLES]\n",
        "    print(\"Keeping\", len(keep), \"classes.\")\n",
        "    # build new reduced folder at /content/plant_disease_pruned\n",
        "    import shutil\n",
        "    PRUNED = Path('/content/plant_disease_pruned')\n",
        "    if PRUNED.exists(): shutil.rmtree(PRUNED)\n",
        "    PRUNED.mkdir(parents=True, exist_ok=True)\n",
        "    for split in ['train','val','test']:\n",
        "        (PRUNED/split).mkdir(parents=True, exist_ok=True)\n",
        "        for c in keep:\n",
        "            src = BASE/ split / c\n",
        "            if src.exists():\n",
        "                shutil.copytree(src, PRUNED/split/c)\n",
        "    print(\"Pruned dataset created at\", PRUNED)\n",
        "    # If you pruned, re-create datasets from PRUNED path (update DATA_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xY_7bdWKiQYh",
        "outputId": "dae47598-4026-4ab3-bd30-7a7e0114c935"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 40 classes by total image count:\n",
            "  GCREC_Bact.Sp 5152\n",
            "  Com.G_SpM_FL 4822\n",
            "  UF.GRC_YLCV_Lab 4572\n",
            "  JR_HL 4370\n",
            "  Com.G_TgS_FL 4122\n",
            "  RS_HL 3344\n",
            "  RS_LB 2962\n",
            "  RS_Early.B 2960\n",
            "  Matt.S_CG 2956\n",
            "  RS_Erly.B 2904\n",
            "  Crnl_L.Mold 2818\n",
            "  RS_Late.B 2686\n",
            "  YLCV_GCREC 2608\n",
            "  YLCV_NREC 2202\n",
            "  GHLB2_Leaf 1804\n",
            "  JR_B.Spot 1786\n",
            "  JR_Sept.L.S 1452\n",
            "  NREC_B.Spot 1188\n",
            "  PSU_CG 1110\n",
            "  UF.GRC_BS_Lab_Leaf 1076\n",
            "  GH_HL_Leaf 938\n",
            "  Keller.St_CG 824\n",
            "  GHLB2ES_Leaf 48\n",
            "  GHLB_PS_Leaf_1_Day 20\n",
            "  GHLB_Leaf_2_Day 16\n",
            "  GHLB_PS_Leaf_24_Day 16\n",
            "  GHLB_PS_Leaf_8.1_Day 14\n",
            "  GHLB_PS_Leaf_1.2_Day 12\n",
            "  GHLB_Leaf_1_Day 10\n",
            "  GHLB_Leaf_2.1_Day 10\n",
            "  GHLB_Leaf_23_Day 10\n",
            "  GHLB_PS_Leaf_1.5_Day 10\n",
            "  GHLB_PS_Leaf_2.1_Day 10\n",
            "  GHLB_PS_Leaf_2_Day 10\n",
            "  GHLB_PS_Leaf_8_Day 10\n",
            "  GHLB_Leaf_1.2_Day 8\n",
            "  GHLB_Leaf_23.1_Day 8\n",
            "  GHLB_Leaf_23.4_Day 8\n",
            "  GHLB_PS_Leaf_2.2_Day 8\n",
            "  GHLB_PS_Leaf_2.3_Day 8\n",
            "\n",
            "Classes with < 20 images: 517\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3 — imports and augmentation layer\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import os\n",
        "\n",
        "IMG_SIZE = (224,224)\n",
        "data_augmentation = keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.05),\n",
        "    layers.RandomZoom(0.05),\n",
        "], name='data_augmentation')\n"
      ],
      "metadata": {
        "id": "Kw9udIGbiQa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4 — baseline CNN (num_classes from dataset)\n",
        "def make_baseline_cnn(input_shape=IMG_SIZE+(3,), num_classes=num_classes):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = data_augmentation(inputs)\n",
        "    x = layers.Rescaling(1./255)(x)\n",
        "    x = layers.Conv2D(32,3,activation='relu',padding='same')(x)\n",
        "    x = layers.MaxPool2D()(x)\n",
        "    x = layers.Conv2D(64,3,activation='relu',padding='same')(x)\n",
        "    x = layers.MaxPool2D()(x)\n",
        "    x = layers.Conv2D(128,3,activation='relu',padding='same')(x)\n",
        "    x = layers.MaxPool2D()(x)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    x = layers.Dense(128, activation='relu')(x)\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "    model = keras.Model(inputs, outputs, name='baseline_cnn')\n",
        "    return model\n",
        "\n",
        "baseline = make_baseline_cnn()\n",
        "baseline.compile(optimizer=keras.optimizers.Adam(1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "baseline.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "id": "zmp1nDOfiQfP",
        "outputId": "2bbbcd71-8385-46cd-b842-6a4c433a2748"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"baseline_cnn\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"baseline_cnn\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_4 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ data_augmentation (\u001b[38;5;33mSequential\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ rescaling_2 (\u001b[38;5;33mRescaling\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_5 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m16,512\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m541\u001b[0m)            │        \u001b[38;5;34m69,789\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ data_augmentation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ rescaling_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">541</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">69,789</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m179,549\u001b[0m (701.36 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">179,549</span> (701.36 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m179,549\u001b[0m (701.36 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">179,549</span> (701.36 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rebuild train/val/test datasets with a unified class mapping (fixes label dimension mismatch)\n",
        "import os, json\n",
        "from pathlib import Path\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "BASE = Path('/content/plant_disease_fixed')\n",
        "IMG_SIZE = (224,224)\n",
        "BATCH_SIZE = 32   # reduce to 16 or 8 if OOM\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "IMG_EXTS = {'.jpg','.jpeg','.png','.bmp','.tif','.tiff'}\n",
        "\n",
        "# 1) find union of class folders across train/val/test\n",
        "splits = {'train': BASE/'train', 'val': BASE/'val', 'test': BASE/'test'}\n",
        "class_set = set()\n",
        "for sname, sdir in splits.items():\n",
        "    if sdir.exists():\n",
        "        for d in sdir.iterdir():\n",
        "            if d.is_dir():\n",
        "                class_set.add(d.name)\n",
        "class_names = sorted(list(class_set))\n",
        "num_classes = len(class_names)\n",
        "print(\"Unified num_classes:\", num_classes)\n",
        "\n",
        "# mapping\n",
        "class_to_idx = {c:i for i,c in enumerate(class_names)}\n",
        "\n",
        "# 2) helper: gather files and numeric labels for a split\n",
        "def gather_files_and_labels(split_dir):\n",
        "    files = []\n",
        "    labels = []\n",
        "    sd = Path(split_dir)\n",
        "    if not sd.exists():\n",
        "        return files, labels\n",
        "    for cls in class_names:\n",
        "        folder = sd / cls\n",
        "        if folder.exists():\n",
        "            for f in folder.rglob('*'):\n",
        "                if f.is_file() and f.suffix.lower() in IMG_EXTS:\n",
        "                    files.append(str(f))\n",
        "                    labels.append(class_to_idx[cls])\n",
        "    return files, labels\n",
        "\n",
        "train_files, train_labels = gather_files_and_labels(splits['train'])\n",
        "val_files, val_labels     = gather_files_and_labels(splits['val'])\n",
        "test_files, test_labels   = gather_files_and_labels(splits['test'])\n",
        "\n",
        "print(\"Counts -> train:\", len(train_files), \"val:\", len(val_files), \"test:\", len(test_files))\n",
        "\n",
        "# quick failure check\n",
        "if len(train_files) == 0:\n",
        "    raise RuntimeError(\"No training files found. Check /content/plant_disease_fixed/train\")\n",
        "\n",
        "# 3) build tf.data pipelines from file paths + labels with consistent one-hot labels\n",
        "def make_dataset(files, labels, shuffle=False):\n",
        "    ds = tf.data.Dataset.from_tensor_slices((files, labels))\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size=min(10000, max(1024, len(files))))\n",
        "    def _load(path, label):\n",
        "        img = tf.io.read_file(path)\n",
        "        img = tf.io.decode_image(img, channels=3, expand_animations=False)\n",
        "        img = tf.image.resize(img, IMG_SIZE)\n",
        "        img = tf.cast(img, tf.float32) / 255.0\n",
        "        label = tf.one_hot(label, depth=num_classes)\n",
        "        return img, label\n",
        "    ds = ds.map(_load, num_parallel_calls=AUTOTUNE)\n",
        "    ds = ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "train_ds = make_dataset(train_files, train_labels, shuffle=True)\n",
        "val_ds   = make_dataset(val_files, val_labels, shuffle=False)\n",
        "test_ds  = make_dataset(test_files, test_labels, shuffle=False)\n",
        "\n",
        "# 4) save unified class list to Drive for later\n",
        "MODELS_DIR = '/content/drive/MyDrive/Buildable-ML-DL-Fellowship/models'\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "with open(os.path.join(MODELS_DIR,'classes_unified.json'),'w') as f:\n",
        "    json.dump(class_names, f)\n",
        "\n",
        "# 5) sanity checks\n",
        "print(\"train batches:\", tf.data.experimental.cardinality(train_ds).numpy())\n",
        "print(\"val batches:  \", tf.data.experimental.cardinality(val_ds).numpy())\n",
        "print(\"test batches: \", tf.data.experimental.cardinality(test_ds).numpy())\n",
        "# print first batch shapes\n",
        "for imgs, labs in train_ds.take(1):\n",
        "    print(\"sample batch imgs shape:\", imgs.shape, \"labels shape:\", labs.shape)\n",
        "    break\n",
        "\n",
        "print(\"Unified mapping saved to:\", os.path.join(MODELS_DIR,'classes_unified.json'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRYoXLsGldBm",
        "outputId": "530284be-f293-4312-f023-c03f78894105"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unified num_classes: 587\n",
            "Counts -> train: 37570 val: 11468 test: 11516\n",
            "train batches: 1175\n",
            "val batches:   359\n",
            "test batches:  360\n",
            "sample batch imgs shape: (32, 224, 224, 3) labels shape: (32, 587)\n",
            "Unified mapping saved to: /content/drive/MyDrive/Buildable-ML-DL-Fellowship/models/classes_unified.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell A — load unified classes and confirm num_classes\n",
        "import json, os\n",
        "from pathlib import Path\n",
        "MODELS_DIR = '/content/drive/MyDrive/Buildable-ML-DL-Fellowship/models'\n",
        "cls_file = os.path.join(MODELS_DIR, 'classes_unified.json')\n",
        "\n",
        "if os.path.exists(cls_file):\n",
        "    with open(cls_file, 'r') as f:\n",
        "        class_names = json.load(f)\n",
        "    print(\"Loaded classes_unified.json with\", len(class_names), \"classes\")\n",
        "else:\n",
        "    # fallback: infer from train folders\n",
        "    BASE = Path('/content/plant_disease_fixed')\n",
        "    train_dir = BASE/'train'\n",
        "    class_names = sorted([d.name for d in train_dir.iterdir() if d.is_dir()])\n",
        "    print(\"Inferred class_names from train folder:\", len(class_names), \"classes\")\n",
        "\n",
        "num_classes = len(class_names)\n",
        "print(\"num_classes =\", num_classes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImxjBXl8m5Qx",
        "outputId": "837958d2-e1ab-4071-d782-57818803b08d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded classes_unified.json with 587 classes\n",
            "num_classes = 587\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell B — ensure train_ds/val_ds/test_ds are the unified ones (rebuild if needed)\n",
        "import tensorflow as tf, os\n",
        "from pathlib import Path\n",
        "\n",
        "BASE = Path('/content/plant_disease_fixed')\n",
        "IMG_SIZE = (224,224)\n",
        "BATCH_SIZE = 32   # lower to 16/8 if OOM\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "IMG_EXTS = {'.jpg','.jpeg','.png','.bmp','.tif','.tiff'}\n",
        "\n",
        "# build mapping (class_names must be present from previous cell)\n",
        "class_to_idx = {c:i for i,c in enumerate(class_names)}\n",
        "def gather_files_and_labels(split_dir):\n",
        "    files, labels = [], []\n",
        "    p = Path(split_dir)\n",
        "    if not p.exists(): return files, labels\n",
        "    for cls in class_names:\n",
        "        folder = p/cls\n",
        "        if folder.exists():\n",
        "            for f in folder.rglob('*'):\n",
        "                if f.is_file() and f.suffix.lower() in IMG_EXTS:\n",
        "                    files.append(str(f))\n",
        "                    labels.append(class_to_idx[cls])\n",
        "    return files, labels\n",
        "\n",
        "train_files, train_labels = gather_files_and_labels(BASE/'train')\n",
        "val_files, val_labels     = gather_files_and_labels(BASE/'val')\n",
        "test_files, test_labels   = gather_files_and_labels(BASE/'test')\n",
        "\n",
        "print(\"Counts -> train:\", len(train_files), \"val:\", len(val_files), \"test:\", len(test_files))\n",
        "\n",
        "def make_dataset(files, labels, shuffle=False):\n",
        "    ds = tf.data.Dataset.from_tensor_slices((files, labels))\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size=min(10000, max(1024, len(files))))\n",
        "    def _load(path, label):\n",
        "        img = tf.io.read_file(path)\n",
        "        img = tf.io.decode_image(img, channels=3, expand_animations=False)\n",
        "        img = tf.image.resize(img, IMG_SIZE)\n",
        "        img = tf.cast(img, tf.float32) / 255.0\n",
        "        label = tf.one_hot(label, depth=num_classes)\n",
        "        return img, label\n",
        "    ds = ds.map(_load, num_parallel_calls=AUTOTUNE)\n",
        "    ds = ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "train_ds = make_dataset(train_files, train_labels, shuffle=True)\n",
        "val_ds   = make_dataset(val_files, val_labels, shuffle=False)\n",
        "test_ds  = make_dataset(test_files, test_labels, shuffle=False)\n",
        "\n",
        "# quick sanity\n",
        "for imgs, labs in train_ds.take(1):\n",
        "    print(\"batch imgs\", imgs.shape, \"labels\", labs.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKvVQ76lm5OA",
        "outputId": "98e8bcb0-7097-4426-f879-ef6088ccd938"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counts -> train: 37570 val: 11468 test: 11516\n",
            "batch imgs (32, 224, 224, 3) labels (32, 587)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell C — (re)create baseline using the correct num_classes\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "IMG_SIZE = (224,224)\n",
        "\n",
        "def make_baseline_cnn(input_shape=IMG_SIZE+(3,), num_classes=num_classes):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = layers.RandomFlip(\"horizontal\")(inputs)\n",
        "    x = layers.RandomRotation(0.05)(x)\n",
        "    x = layers.RandomZoom(0.05)(x)\n",
        "    x = layers.Rescaling(1./255)(x)\n",
        "    x = layers.Conv2D(32,3,activation='relu',padding='same')(x)\n",
        "    x = layers.MaxPool2D()(x)\n",
        "    x = layers.Conv2D(64,3,activation='relu',padding='same')(x)\n",
        "    x = layers.MaxPool2D()(x)\n",
        "    x = layers.Conv2D(128,3,activation='relu',padding='same')(x)\n",
        "    x = layers.MaxPool2D()(x)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    x = layers.Dense(128, activation='relu')(x)\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "    model = keras.Model(inputs, outputs, name='baseline_cnn')\n",
        "    return model\n",
        "\n",
        "baseline = make_baseline_cnn()\n",
        "baseline.compile(optimizer=keras.optimizers.Adam(1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "baseline.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "IjUHKgmQm5Lf",
        "outputId": "5abf1611-96a4-4941-90f0-d9444fa60234"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"baseline_cnn\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"baseline_cnn\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_6 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ random_flip_2 (\u001b[38;5;33mRandomFlip\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ random_rotation_2               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mRandomRotation\u001b[0m)                │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ random_zoom_2 (\u001b[38;5;33mRandomZoom\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ rescaling_3 (\u001b[38;5;33mRescaling\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_6 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_7 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_8 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d_2      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m16,512\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m587\u001b[0m)            │        \u001b[38;5;34m75,723\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ random_flip_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RandomFlip</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ random_rotation_2               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RandomRotation</span>)                │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ random_zoom_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RandomZoom</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ rescaling_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d_2      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">587</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">75,723</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m185,483\u001b[0m (724.54 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">185,483</span> (724.54 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m185,483\u001b[0m (724.54 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">185,483</span> (724.54 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell D — train baseline (same callbacks as before)\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "import os\n",
        "MODELS_DIR = '/content/drive/MyDrive/Buildable-ML-DL-Fellowship/models'\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "\n",
        "baseline_ckpt = os.path.join(MODELS_DIR, 'disease_baseline_best.h5')\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True),\n",
        "    ModelCheckpoint(baseline_ckpt, monitor='val_loss', save_best_only=True),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
        "]\n",
        "\n",
        "EPOCHS = 20\n",
        "history_baseline = baseline.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, callbacks=callbacks)\n",
        "baseline.save(os.path.join(MODELS_DIR,'disease_baseline.h5'))\n",
        "print(\"Saved baseline model to\", MODELS_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbVUVSNOm5Iw",
        "outputId": "70b62d70-fbdb-43ab-f3f7-56f6905f94b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.1953 - loss: 3.1000"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 59ms/step - accuracy: 0.1953 - loss: 3.0999 - val_accuracy: 0.0778 - val_loss: 3.6617 - learning_rate: 0.0010\n",
            "Epoch 2/20\n",
            "\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.1848 - loss: 2.6573"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 58ms/step - accuracy: 0.1848 - loss: 2.6573 - val_accuracy: 0.0991 - val_loss: 3.4346 - learning_rate: 0.0010\n",
            "Epoch 3/20\n",
            "\u001b[1m1174/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.2730 - loss: 2.4922"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 57ms/step - accuracy: 0.2729 - loss: 2.4922 - val_accuracy: 0.1071 - val_loss: 3.3060 - learning_rate: 0.0010\n",
            "Epoch 4/20\n",
            "\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.2933 - loss: 2.4154"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 58ms/step - accuracy: 0.2933 - loss: 2.4155 - val_accuracy: 0.1317 - val_loss: 3.2322 - learning_rate: 0.0010\n",
            "Epoch 5/20\n",
            "\u001b[1m1174/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.3157 - loss: 2.3348"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 58ms/step - accuracy: 0.3157 - loss: 2.3349 - val_accuracy: 0.1828 - val_loss: 3.0932 - learning_rate: 0.0010\n",
            "Epoch 6/20\n",
            "\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.3614 - loss: 2.2032"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 56ms/step - accuracy: 0.3614 - loss: 2.2032 - val_accuracy: 0.1765 - val_loss: 3.0397 - learning_rate: 0.0010\n",
            "Epoch 7/20\n",
            "\u001b[1m1174/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.3712 - loss: 2.1060"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 59ms/step - accuracy: 0.3711 - loss: 2.1062 - val_accuracy: 0.1918 - val_loss: 2.9746 - learning_rate: 0.0010\n",
            "Epoch 8/20\n",
            "\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.3722 - loss: 2.0811"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 57ms/step - accuracy: 0.3722 - loss: 2.0811 - val_accuracy: 0.1981 - val_loss: 2.9161 - learning_rate: 0.0010\n",
            "Epoch 9/20\n",
            "\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 57ms/step - accuracy: 0.3862 - loss: 2.0531 - val_accuracy: 0.1972 - val_loss: 2.9280 - learning_rate: 0.0010\n",
            "Epoch 10/20\n",
            "\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.3940 - loss: 2.0015"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 57ms/step - accuracy: 0.3940 - loss: 2.0015 - val_accuracy: 0.2039 - val_loss: 2.8680 - learning_rate: 0.0010\n",
            "Epoch 11/20\n",
            "\u001b[1m1174/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.4009 - loss: 1.9315"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 57ms/step - accuracy: 0.4009 - loss: 1.9316 - val_accuracy: 0.2122 - val_loss: 2.8106 - learning_rate: 0.0010\n",
            "Epoch 12/20\n",
            "\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 57ms/step - accuracy: 0.3993 - loss: 1.9374 - val_accuracy: 0.2107 - val_loss: 2.8473 - learning_rate: 0.0010\n",
            "Epoch 13/20\n",
            "\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.4106 - loss: 1.9105"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 57ms/step - accuracy: 0.4106 - loss: 1.9106 - val_accuracy: 0.2349 - val_loss: 2.7738 - learning_rate: 0.0010\n",
            "Epoch 14/20\n",
            "\u001b[1m1174/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.4165 - loss: 1.8749"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 57ms/step - accuracy: 0.4165 - loss: 1.8751 - val_accuracy: 0.2417 - val_loss: 2.7507 - learning_rate: 0.0010\n",
            "Epoch 15/20\n",
            "\u001b[1m1174/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.4259 - loss: 1.8538"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 57ms/step - accuracy: 0.4259 - loss: 1.8539 - val_accuracy: 0.2450 - val_loss: 2.7286 - learning_rate: 0.0010\n",
            "Epoch 16/20\n",
            "\u001b[1m1174/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.4257 - loss: 1.8281"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 59ms/step - accuracy: 0.4257 - loss: 1.8283 - val_accuracy: 0.2360 - val_loss: 2.7124 - learning_rate: 0.0010\n",
            "Epoch 17/20\n",
            "\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.4328 - loss: 1.7967"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 58ms/step - accuracy: 0.4328 - loss: 1.7968 - val_accuracy: 0.2513 - val_loss: 2.6967 - learning_rate: 0.0010\n",
            "Epoch 18/20\n",
            "\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.4462 - loss: 1.7680"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 57ms/step - accuracy: 0.4462 - loss: 1.7681 - val_accuracy: 0.2478 - val_loss: 2.6520 - learning_rate: 0.0010\n",
            "Epoch 19/20\n",
            "\u001b[1m1174/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.4529 - loss: 1.7308"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 57ms/step - accuracy: 0.4529 - loss: 1.7309 - val_accuracy: 0.2769 - val_loss: 2.5106 - learning_rate: 0.0010\n",
            "Epoch 20/20\n",
            "\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.4613 - loss: 1.7006"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 58ms/step - accuracy: 0.4613 - loss: 1.7007 - val_accuracy: 0.2865 - val_loss: 2.4972 - learning_rate: 0.0010\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved baseline model to /content/drive/MyDrive/Buildable-ML-DL-Fellowship/models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell E — build ResNet model using same num_classes (train top, then fine-tune)\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import os\n",
        "\n",
        "def make_resnet_model(input_shape=IMG_SIZE+(3,), num_classes=num_classes, base_trainable=False):\n",
        "    base = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape, pooling='avg')\n",
        "    base.trainable = base_trainable\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = layers.RandomFlip(\"horizontal\")(inputs)\n",
        "    x = layers.RandomRotation(0.05)(x)\n",
        "    x = layers.RandomZoom(0.05)(x)\n",
        "    x = layers.Rescaling(1./255)(x)\n",
        "    x = base(x, training=False)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    x = layers.Dense(256, activation='relu')(x)\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "    model = keras.Model(inputs, outputs, name='resnet50_transfer')\n",
        "    return model\n",
        "\n",
        "resnet_model = make_resnet_model(base_trainable=False)\n",
        "resnet_model.compile(optimizer=keras.optimizers.Adam(1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "resnet_model.summary()\n",
        "\n",
        "# Train top layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "MODELS_DIR = '/content/drive/MyDrive/Buildable-ML-DL-Fellowship/models'\n",
        "resnet_ckpt = os.path.join(MODELS_DIR, 'disease_resnet50_top_best.h5')\n",
        "callbacks = [EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True),\n",
        "             ModelCheckpoint(resnet_ckpt, monitor='val_loss', save_best_only=True),\n",
        "             ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)]\n",
        "history_resnet_top = resnet_model.fit(train_ds, validation_data=val_ds, epochs=10, callbacks=callbacks)\n",
        "resnet_model.save(os.path.join(MODELS_DIR,'disease_resnet50_top.h5'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        },
        "id": "-IPGUMhxm5GS",
        "outputId": "88b2963c-f098-444e-9266-16423f85831a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"resnet50_transfer\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"resnet50_transfer\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_8 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ random_flip_3 (\u001b[38;5;33mRandomFlip\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ random_rotation_3               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mRandomRotation\u001b[0m)                │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ random_zoom_3 (\u001b[38;5;33mRandomZoom\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ rescaling_4 (\u001b[38;5;33mRescaling\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ resnet50 (\u001b[38;5;33mFunctional\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │    \u001b[38;5;34m23,587,712\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m524,544\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m587\u001b[0m)            │       \u001b[38;5;34m150,859\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ random_flip_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RandomFlip</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ random_rotation_3               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RandomRotation</span>)                │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ random_zoom_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RandomZoom</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ rescaling_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ resnet50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │    <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,544</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">587</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">150,859</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m24,263,115\u001b[0m (92.56 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,263,115</span> (92.56 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m675,403\u001b[0m (2.58 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">675,403</span> (2.58 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m23,587,712\u001b[0m (89.98 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> (89.98 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.1965 - loss: 2.7374"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 172ms/step - accuracy: 0.1965 - loss: 2.7375 - val_accuracy: 0.0778 - val_loss: 3.8467 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m1174/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.1831 - loss: 2.6440"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 170ms/step - accuracy: 0.1831 - loss: 2.6442 - val_accuracy: 0.0778 - val_loss: 3.5847 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 167ms/step - accuracy: 0.1861 - loss: 2.6840 - val_accuracy: 0.0778 - val_loss: 3.6888 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 167ms/step - accuracy: 0.1791 - loss: 2.7483 - val_accuracy: 0.0778 - val_loss: 3.7202 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 167ms/step - accuracy: 0.1754 - loss: 2.7577 - val_accuracy: 0.0778 - val_loss: 3.6885 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 167ms/step - accuracy: 0.1421 - loss: 3.0354 - val_accuracy: 0.0778 - val_loss: 3.7200 - learning_rate: 5.0000e-04\n",
            "Epoch 7/10\n",
            "\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 167ms/step - accuracy: 0.1510 - loss: 2.8677 - val_accuracy: 0.0778 - val_loss: 3.6783 - learning_rate: 5.0000e-04\n",
            "Epoch 8/10\n",
            "\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 167ms/step - accuracy: 0.1776 - loss: 2.7410 - val_accuracy: 0.0778 - val_loss: 3.7173 - learning_rate: 5.0000e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell F — unfreeze last N layers and fine-tune\n",
        "# find base by name\n",
        "base_model = None\n",
        "for layer in resnet_model.layers:\n",
        "    if 'resnet50' in layer.name:\n",
        "        base_model = layer\n",
        "        break\n",
        "if base_model is None:\n",
        "    base_model = resnet_model.layers[3]\n",
        "\n",
        "N = 20\n",
        "for layer in base_model.layers[:-N]:\n",
        "    layer.trainable = False\n",
        "for layer in base_model.layers[-N:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "resnet_model.compile(optimizer=keras.optimizers.Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history_resnet_ft = resnet_model.fit(train_ds, validation_data=val_ds, epochs=10, callbacks=[\n",
        "    EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True),\n",
        "    ModelCheckpoint(os.path.join(MODELS_DIR,'disease_resnet50_ft_best.h5'), monitor='val_loss', save_best_only=True),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
        "])\n",
        "resnet_model.save(os.path.join(MODELS_DIR,'disease_resnet50.h5'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2IP2FsIm5Dp",
        "outputId": "38fa32c4-585d-4de0-dee9-f8571da5f52c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.1249 - loss: 2.7929"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m246s\u001b[0m 201ms/step - accuracy: 0.1249 - loss: 2.7929 - val_accuracy: 0.0778 - val_loss: 3.7034 - learning_rate: 1.0000e-05\n",
            "Epoch 2/10\n",
            "\u001b[1m1174/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.1608 - loss: 2.8320"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 198ms/step - accuracy: 0.1608 - loss: 2.8319 - val_accuracy: 0.0778 - val_loss: 3.6992 - learning_rate: 1.0000e-05\n",
            "Epoch 3/10\n",
            "\u001b[1m1174/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.1797 - loss: 2.6868"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 197ms/step - accuracy: 0.1797 - loss: 2.6870 - val_accuracy: 0.0778 - val_loss: 3.6526 - learning_rate: 1.0000e-05\n",
            "Epoch 4/10\n",
            "\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 194ms/step - accuracy: 0.1733 - loss: 2.7219 - val_accuracy: 0.0778 - val_loss: 3.7027 - learning_rate: 1.0000e-05\n",
            "Epoch 5/10\n",
            "\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 194ms/step - accuracy: 0.1819 - loss: 2.6875 - val_accuracy: 0.0778 - val_loss: 3.7469 - learning_rate: 1.0000e-05\n",
            "Epoch 6/10\n",
            "\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 227ms/step - accuracy: 0.1801 - loss: 2.6573 - val_accuracy: 0.0778 - val_loss: 3.7552 - learning_rate: 1.0000e-05\n",
            "Epoch 7/10\n",
            "\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 193ms/step - accuracy: 0.1657 - loss: 2.7664 - val_accuracy: 0.0778 - val_loss: 3.6917 - learning_rate: 5.0000e-06\n",
            "Epoch 8/10\n",
            "\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 194ms/step - accuracy: 0.1707 - loss: 2.7308 - val_accuracy: 0.0778 - val_loss: 3.6585 - learning_rate: 5.0000e-06\n",
            "Epoch 9/10\n",
            "\u001b[1m1174/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.1734 - loss: 2.7191"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 199ms/step - accuracy: 0.1734 - loss: 2.7192 - val_accuracy: 0.0778 - val_loss: 3.6515 - learning_rate: 5.0000e-06\n",
            "Epoch 10/10\n",
            "\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 194ms/step - accuracy: 0.1769 - loss: 2.7158 - val_accuracy: 0.0778 - val_loss: 3.6642 - learning_rate: 5.0000e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EfficientNetB0 + Focal Loss + Strong Augmentation training cell\n",
        "import tensorflow as tf, os, json, time\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "MODELS_DIR = '/content/drive/MyDrive/Buildable-ML-DL-Fellowship/models'\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "\n",
        "IMG_SIZE = (224,224)\n",
        "BATCH_SIZE = 24   # try 24; if OOM, drop to 16 or 12\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ],
      "metadata": {
        "id": "wg-omDfJB1MY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load unified classes (must exist)\n",
        "with open(os.path.join(MODELS_DIR,'classes_unified.json'),'r') as f:\n",
        "    class_names = json.load(f)\n",
        "num_classes = len(class_names)\n",
        "print(\"num_classes:\", num_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ay8A7eckDV9u",
        "outputId": "72eeea6a-d9ff-47b7-dbda-028d6a574882"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_classes: 587\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- strong augmentation (training-only)\n",
        "data_augmentation = keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.08),\n",
        "    layers.RandomZoom(0.08),\n",
        "    layers.RandomContrast(0.15),\n",
        "], name='strong_augmentation')"
      ],
      "metadata": {
        "id": "fxKovZEmDWDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- focal loss (categorical)\n",
        "def categorical_focal_loss(gamma=2.0, alpha=0.25):\n",
        "    # expects probabilities (after softmax) and one-hot ground truth\n",
        "    def loss_fn(y_true, y_pred):\n",
        "        # clip to avoid NaNs\n",
        "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0 - 1e-7)\n",
        "        cross_entropy = -y_true * tf.math.log(y_pred)\n",
        "        weight = alpha * tf.pow(1 - y_pred, gamma)\n",
        "        loss = weight * cross_entropy\n",
        "        return tf.reduce_sum(loss, axis=-1)\n",
        "    return loss_fn"
      ],
      "metadata": {
        "id": "8je6UHOxDWKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Build model\n",
        "def make_efficientnet(input_shape=IMG_SIZE+(3,), num_classes=num_classes, base_trainable=False):\n",
        "    base = tf.keras.applications.EfficientNetB0(\n",
        "        include_top=False, weights='imagenet', input_shape=input_shape, pooling='avg'\n",
        "    )\n",
        "    base.trainable = base_trainable\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = data_augmentation(inputs)\n",
        "    x = layers.Rescaling(1./255)(x)\n",
        "    x = base(x, training=False)\n",
        "    x = layers.Dropout(0.4)(x)\n",
        "    x = layers.Dense(512, activation='swish')(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "    model = keras.Model(inputs, outputs, name='efficientnetb0_transfer')\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "i_iKR_tnDWSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make model (head training)\n",
        "model = make_efficientnet(base_trainable=False)\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss=categorical_focal_loss(gamma=2.0, alpha=0.25),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "iHjhqES_DWna",
        "outputId": "e4400baa-406d-4bdc-dd26-318fa13572cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"efficientnetb0_transfer\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"efficientnetb0_transfer\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_10 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ strong_augmentation             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mSequential\u001b[0m)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ rescaling_7 (\u001b[38;5;33mRescaling\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ efficientnetb0 (\u001b[38;5;33mFunctional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │     \u001b[38;5;34m4,049,571\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m655,872\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m587\u001b[0m)            │       \u001b[38;5;34m301,131\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ strong_augmentation             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ rescaling_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ efficientnetb0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">655,872</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">587</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">301,131</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,006,574\u001b[0m (19.10 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,006,574</span> (19.10 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m957,003\u001b[0m (3.65 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">957,003</span> (3.65 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,049,571\u001b[0m (15.45 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> (15.45 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Callbacks\n",
        "callbacks = [\n",
        "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True),\n",
        "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7),\n",
        "    keras.callbacks.ModelCheckpoint(os.path.join(MODELS_DIR,'disease_efficientnet_head_best.h5'), monitor='val_loss', save_best_only=True)\n",
        "]\n",
        "\n",
        "# train head\n",
        "EPOCHS_HEAD = 8\n",
        "history_head = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS_HEAD, callbacks=callbacks)\n",
        "\n",
        "# Save head\n",
        "model.save(os.path.join(MODELS_DIR,'disease_efficientnet_head.h5'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21nF3riWC75J",
        "outputId": "49455c11-f552-49f4-d43e-6f290db58b00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8\n",
            "\u001b[1m1174/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.1839 - loss: 0.5706"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 116ms/step - accuracy: 0.1839 - loss: 0.5707 - val_accuracy: 0.0778 - val_loss: 0.8796 - learning_rate: 0.0010\n",
            "Epoch 2/8\n",
            "\u001b[1m1174/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.1919 - loss: 0.5420"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 103ms/step - accuracy: 0.1919 - loss: 0.5421 - val_accuracy: 0.0778 - val_loss: 0.8120 - learning_rate: 0.0010\n",
            "Epoch 3/8\n",
            "\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 115ms/step - accuracy: 0.1932 - loss: 0.5290 - val_accuracy: 0.0778 - val_loss: 0.8819 - learning_rate: 0.0010\n",
            "Epoch 4/8\n",
            "\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 101ms/step - accuracy: 0.1861 - loss: 0.5411 - val_accuracy: 0.0778 - val_loss: 0.8693 - learning_rate: 0.0010\n",
            "Epoch 5/8\n",
            "\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 102ms/step - accuracy: 0.1896 - loss: 0.5419 - val_accuracy: 0.0778 - val_loss: 0.8891 - learning_rate: 0.0010\n",
            "Epoch 6/8\n",
            "\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 115ms/step - accuracy: 0.1851 - loss: 0.5552 - val_accuracy: 0.0778 - val_loss: 0.8685 - learning_rate: 5.0000e-04\n",
            "Epoch 7/8\n",
            "\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 102ms/step - accuracy: 0.1856 - loss: 0.5482 - val_accuracy: 0.0778 - val_loss: 0.8378 - learning_rate: 5.0000e-04\n",
            "Epoch 8/8\n",
            "\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 101ms/step - accuracy: 0.1863 - loss: 0.5498 - val_accuracy: 0.0778 - val_loss: 0.8967 - learning_rate: 5.0000e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Fine-tune: unfreeze last blocks of EfficientNet base\n",
        "# get base\n",
        "base = None\n",
        "for layer in model.layers:\n",
        "    if 'efficientnet' in layer.name or 'efficientnetb0' in layer.name.lower():\n",
        "        base = layer\n",
        "        break\n",
        "# fallback: find first functional layer with many layers\n",
        "if base is None:\n",
        "    base = model.layers[3]\n",
        "\n",
        "# Unfreeze last N layers of base; choose N depending on available GPU/memory\n",
        "N = 60   # number of layers from the end of the base to unfreeze; lower if OOM\n",
        "print(\"Total layers in base:\", len(base.layers))\n",
        "for layer in base.layers[:-N]:\n",
        "    layer.trainable = False\n",
        "for layer in base.layers[-N:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# recompile with lower LR\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n",
        "    loss=categorical_focal_loss(gamma=2.0, alpha=0.25),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# fine-tune\n",
        "EPOCHS_FINE = 12\n",
        "callbacks_ft = [\n",
        "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True),\n",
        "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7),\n",
        "    keras.callbacks.ModelCheckpoint(os.path.join(MODELS_DIR,'disease_efficientnet_ft_best.h5'), monitor='val_loss', save_best_only=True)\n",
        "]\n",
        "\n",
        "history_ft = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS_FINE, callbacks=callbacks_ft)\n",
        "\n",
        "# Save final model\n",
        "model.save(os.path.join(MODELS_DIR,'disease_efficientnet_final.h5'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPYNWZ1TItHm",
        "outputId": "e30d34dc-679b-4ae2-fa72-d8700da75aab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total layers in base: 239\n",
            "Epoch 1/12\n",
            "\u001b[1m1174/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.0611 - loss: 2.2432"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 135ms/step - accuracy: 0.0612 - loss: 2.2423 - val_accuracy: 0.0778 - val_loss: 0.8649 - learning_rate: 1.0000e-05\n",
            "Epoch 2/12\n",
            "\u001b[1m1174/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.0786 - loss: 1.2703"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 133ms/step - accuracy: 0.0787 - loss: 1.2697 - val_accuracy: 0.0778 - val_loss: 0.8506 - learning_rate: 1.0000e-05\n",
            "Epoch 3/12\n",
            "\u001b[1m1174/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.0941 - loss: 0.7587"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 132ms/step - accuracy: 0.0941 - loss: 0.7586 - val_accuracy: 0.0778 - val_loss: 0.8489 - learning_rate: 1.0000e-05\n",
            "Epoch 4/12\n",
            "\u001b[1m1174/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.0979 - loss: 0.7464"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 145ms/step - accuracy: 0.0980 - loss: 0.7463 - val_accuracy: 0.0778 - val_loss: 0.8479 - learning_rate: 1.0000e-05\n",
            "Epoch 5/12\n",
            "\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 144ms/step - accuracy: 0.1178 - loss: 0.7219 - val_accuracy: 0.0778 - val_loss: 0.8532 - learning_rate: 1.0000e-05\n",
            "Epoch 6/12\n",
            "\u001b[1m1174/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.1076 - loss: 0.7287"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 131ms/step - accuracy: 0.1076 - loss: 0.7286 - val_accuracy: 0.0778 - val_loss: 0.8423 - learning_rate: 1.0000e-05\n",
            "Epoch 7/12\n",
            "\u001b[1m1174/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.1105 - loss: 0.7097"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 132ms/step - accuracy: 0.1106 - loss: 0.7097 - val_accuracy: 0.0778 - val_loss: 0.8322 - learning_rate: 1.0000e-05\n",
            "Epoch 8/12\n",
            "\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 130ms/step - accuracy: 0.1247 - loss: 0.6940 - val_accuracy: 0.0778 - val_loss: 0.8363 - learning_rate: 1.0000e-05\n",
            "Epoch 9/12\n",
            "\u001b[1m1174/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.1273 - loss: 0.6838"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 145ms/step - accuracy: 0.1273 - loss: 0.6837 - val_accuracy: 0.0778 - val_loss: 0.8147 - learning_rate: 1.0000e-05\n",
            "Epoch 10/12\n",
            "\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 131ms/step - accuracy: 0.1392 - loss: 0.6622 - val_accuracy: 0.0778 - val_loss: 0.8333 - learning_rate: 1.0000e-05\n",
            "Epoch 11/12\n",
            "\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 131ms/step - accuracy: 0.1349 - loss: 0.6733 - val_accuracy: 0.0778 - val_loss: 0.8261 - learning_rate: 1.0000e-05\n",
            "Epoch 12/12\n",
            "\u001b[1m1175/1175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 143ms/step - accuracy: 0.1444 - loss: 0.6496 - val_accuracy: 0.0778 - val_loss: 0.8381 - learning_rate: 1.0000e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save combined training curves\n",
        "import matplotlib.pyplot as plt\n",
        "def save_history_plots(h, name):\n",
        "    plt.figure(figsize=(8,4)); plt.plot(h.history['loss'], label='train_loss'); plt.plot(h.history['val_loss'], label='val_loss'); plt.legend(); plt.title(name+'_loss'); plt.savefig(os.path.join(MODELS_DIR,name+'_loss.png')); plt.close()\n",
        "    if 'accuracy' in h.history:\n",
        "        plt.figure(figsize=(8,4)); plt.plot(h.history['accuracy'], label='train_acc'); plt.plot(h.history['val_accuracy'], label='val_acc'); plt.legend(); plt.title(name+'_acc'); plt.savefig(os.path.join(MODELS_DIR,name+'_acc.png')); plt.close()\n",
        "\n",
        "save_history_plots(history_head, 'efficientnet_head')\n",
        "save_history_plots(history_ft,   'efficientnet_finetune')"
      ],
      "metadata": {
        "id": "6LjrXS3DB10T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save metrics: evaluate on test\n",
        "loss, acc = model.evaluate(test_ds)\n",
        "metrics = {'test_loss': float(loss), 'test_acc': float(acc)}\n",
        "import json\n",
        "with open(os.path.join(MODELS_DIR,'disease_efficientnet_metrics.json'),'w') as f:\n",
        "    json.dump(metrics, f)\n",
        "print(\"Final test loss, acc:\", loss, acc)\n",
        "print(\"Saved final model and metrics to\", MODELS_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFB1tHiHSmUE",
        "outputId": "69430070-8567-48f3-b05b-97d104676799"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 74ms/step - accuracy: 0.0098 - loss: 0.9084\n",
            "Final test loss, acc: 1.1525895595550537 0.04567558318376541\n",
            "Saved final model and metrics to /content/drive/MyDrive/Buildable-ML-DL-Fellowship/models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fixed Eval Cell — classification report + confusion matrix (robust to missing classes)\n",
        "import os, json, numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "\n",
        "MODELS_DIR = '/content/drive/MyDrive/Buildable-ML-DL-Fellowship/models'\n",
        "MODEL_PATH = os.path.join(MODELS_DIR, \"disease_efficientnet_final.h5\")\n",
        "CLASSES_PATH = os.path.join(MODELS_DIR, \"classes_unified.json\")\n",
        "\n",
        "# load model & classes\n",
        "model = tf.keras.models.load_model(MODEL_PATH, compile=False)\n",
        "with open(CLASSES_PATH, \"r\") as f:\n",
        "    class_names = json.load(f)\n",
        "num_classes = len(class_names)\n",
        "print(\"Loaded model and\", num_classes, \"classes\")\n",
        "\n",
        "# collect predictions and true labels from test_ds (assumes test_ds yields (images, one-hot labels))\n",
        "y_true = []\n",
        "y_pred = []\n",
        "for images, labels in test_ds:\n",
        "    preds = model.predict(images, verbose=0)\n",
        "    y_pred.extend(np.argmax(preds, axis=1).tolist())\n",
        "    y_true.extend(np.argmax(labels.numpy(), axis=1).tolist())\n",
        "\n",
        "y_true = np.array(y_true)\n",
        "y_pred = np.array(y_pred)\n",
        "print(\"Total test samples:\", len(y_true))\n",
        "unique_true = np.unique(y_true)\n",
        "print(\"Unique class indices in test set:\", len(unique_true))\n",
        "\n",
        "# Full classification report (labels = full range so target_names length matches)\n",
        "labels_all = list(range(num_classes))\n",
        "report_full = classification_report(y_true, y_pred, labels=labels_all, target_names=class_names, zero_division=0)\n",
        "with open(os.path.join(MODELS_DIR,'disease_classification_report_full.txt'),'w') as f:\n",
        "    f.write(report_full)\n",
        "print(\"Saved full classification report to disease_classification_report_full.txt\")\n",
        "\n",
        "# Also save a reduced report for only classes present in test (more readable)\n",
        "labels_present = sorted(list(unique_true.tolist()))\n",
        "target_names_present = [class_names[i] for i in labels_present]\n",
        "report_present = classification_report(y_true, y_pred, labels=labels_present, target_names=target_names_present, zero_division=0)\n",
        "with open(os.path.join(MODELS_DIR,'disease_classification_report_test_present.txt'),'w') as f:\n",
        "    f.write(report_present)\n",
        "print(\"Saved condensed report (only classes present in test) to disease_classification_report_test_present.txt\")\n",
        "print(\"\\n--- Sample of condensed report ---\\n\")\n",
        "print(\"\\n\".join(report_present.splitlines()[:40]))\n",
        "\n",
        "# Confusion matrix for top-K frequent classes in test set\n",
        "from collections import Counter\n",
        "cnt = Counter(y_true)\n",
        "topk = [c for c,_ in cnt.most_common(30)]  # change 30 -> smaller if you prefer\n",
        "cm = confusion_matrix(y_true, y_pred, labels=topk)\n",
        "fig, ax = plt.subplots(figsize=(12,10))\n",
        "sns.heatmap(cm, annot=False, fmt='d', ax=ax, cmap='viridis')\n",
        "ax.set_title(\"Confusion matrix for top-{} classes (by frequency)\".format(len(topk)))\n",
        "ax.set_xticks(np.arange(len(topk))+0.5); ax.set_yticks(np.arange(len(topk))+0.5)\n",
        "ax.set_xticklabels([class_names[i] for i in topk], rotation=90, fontsize=8)\n",
        "ax.set_yticklabels([class_names[i] for i in topk], rotation=0, fontsize=8)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(MODELS_DIR,'confusion_matrix_top{}_test.png'.format(len(topk))))\n",
        "plt.close()\n",
        "print(\"Saved confusion matrix for top-{} classes.\".format(len(topk)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rqmq1gKFZJY_",
        "outputId": "4834cdac-f254-4c72-ff7d-2e0ba15b6036"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded model and 587 classes\n",
            "Total test samples: 11516\n",
            "Unique class indices in test set: 181\n",
            "Saved full classification report to disease_classification_report_full.txt\n",
            "Saved condensed report (only classes present in test) to disease_classification_report_test_present.txt\n",
            "\n",
            "--- Sample of condensed report ---\n",
            "\n",
            "                          precision    recall  f1-score   support\n",
            "\n",
            " 2700323949_95aa2eaa01_o       0.00      0.00      0.00         2\n",
            "            Com.G_SpM_FL       0.00      0.00      0.00       864\n",
            "            Com.G_TgS_FL       0.00      0.00      0.00       806\n",
            "             Crnl_L.Mold       0.00      0.00      0.00       530\n",
            "           GCREC_Bact.Sp       0.00      0.00      0.00      1002\n",
            "            GHLB2ES_Leaf       0.00      0.00      0.00         8\n",
            "      GHLB2ES_Leaf_136.1       0.00      0.00      0.00         2\n",
            "      GHLB2ES_Leaf_141.1       0.00      0.00      0.00         2\n",
            "       GHLB2ES_Leaf_69.1       0.00      0.00      0.00         2\n",
            "              GHLB2_Leaf       0.00      0.00      0.00       340\n",
            "        GHLB2_Leaf_100.1       0.00      0.00      0.00         2\n",
            "        GHLB2_Leaf_107.2       0.00      0.00      0.00         2\n",
            "        GHLB2_Leaf_108.1       0.00      0.00      0.00         2\n",
            "        GHLB2_Leaf_110.1       0.00      0.00      0.00         2\n",
            "        GHLB2_Leaf_112.2       0.00      0.00      0.00         2\n",
            "        GHLB2_Leaf_113.3       0.00      0.00      0.00         2\n",
            "        GHLB2_Leaf_115.2       0.00      0.00      0.00         2\n",
            "        GHLB2_Leaf_116.1       0.00      0.00      0.00         2\n",
            "        GHLB2_Leaf_117.4       0.00      0.00      0.00         2\n",
            "        GHLB2_Leaf_121.2       0.00      0.00      0.00         2\n",
            "        GHLB2_Leaf_128.1       0.00      0.00      0.00         2\n",
            "        GHLB2_Leaf_132.1       0.00      0.00      0.00         2\n",
            "        GHLB2_Leaf_143.1       0.00      0.00      0.00         2\n",
            "        GHLB2_Leaf_145.1       0.00      0.00      0.00         2\n",
            "        GHLB2_Leaf_149.2       0.00      0.00      0.00         2\n",
            "        GHLB2_Leaf_154.3       0.00      0.00      0.00         2\n",
            "        GHLB2_Leaf_154.5       0.00      0.00      0.00         2\n",
            "        GHLB2_Leaf_164.1       0.00      0.00      0.00         2\n",
            "         GHLB2_Leaf_55.1       0.00      0.00      0.00         2\n",
            "         GHLB2_Leaf_58.1       0.00      0.00      0.00         2\n",
            "         GHLB2_Leaf_77.1       0.00      0.00      0.00         2\n",
            "         GHLB2_Leaf_78.1       0.00      0.00      0.00         2\n",
            "         GHLB2_Leaf_81.1       0.00      0.00      0.00         2\n",
            "         GHLB2_Leaf_83.1       0.00      0.00      0.00         2\n",
            "         GHLB2_Leaf_91.1       0.00      0.00      0.00         2\n",
            "         GHLB2_Leaf_94.1       0.00      0.00      0.00         2\n",
            "         GHLB2_Leaf_98.1       0.00      0.00      0.00         2\n",
            "       GHLB_Leaf_1.2_Day       0.00      0.00      0.00         4\n",
            "Saved confusion matrix for top-30 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 2 — Inference speed benchmark\n",
        "import time, json\n",
        "\n",
        "# take one batch\n",
        "images, labels = next(iter(test_ds))\n",
        "\n",
        "n_runs = 30\n",
        "t0 = time.perf_counter()\n",
        "\n",
        "for _ in range(n_runs):\n",
        "    _ = model.predict(images, verbose=0)\n",
        "\n",
        "t1 = time.perf_counter()\n",
        "\n",
        "avg_ms_per_image = ((t1 - t0) / n_runs) * 1000 / images.shape[0]\n",
        "print(\"Avg ms per image =\", avg_ms_per_image)\n",
        "\n",
        "# save metrics together\n",
        "metrics = {\n",
        "    \"avg_inference_ms_per_image\": float(avg_ms_per_image)\n",
        "}\n",
        "\n",
        "with open(os.path.join(MODELS_DIR, \"disease_inference_metrics.json\"), \"w\") as f:\n",
        "    json.dump(metrics, f)\n",
        "\n",
        "print(\"Saved inference metrics.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJSHp67JZeB5",
        "outputId": "bd7d9a0e-1843-48c2-f7b9-5af21cc1f9f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg ms per image = 8.93150402291667\n",
            "Saved inference metrics.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Corrected CELL 3 — Create disease_tool.py (safe; avoids .format braces problem)\n",
        "import os, json, textwrap\n",
        "\n",
        "MODELS_DIR = '/content/drive/MyDrive/Buildable-ML-DL-Fellowship/models'\n",
        "MODEL_FILE = os.path.join(MODELS_DIR, \"disease_efficientnet_final.h5\")\n",
        "CLASSES_FILE = os.path.join(MODELS_DIR, \"classes_unified.json\")\n",
        "OUT_PATH = \"/content/disease_tool.py\"\n",
        "\n",
        "code = r'''\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import json\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from pathlib import Path\n",
        "\n",
        "MODEL_PATH = r\"{MODEL_PATH}\"\n",
        "CLASSES_PATH = r\"{CLASSES_PATH}\"\n",
        "\n",
        "# load model and classes once\n",
        "_model = None\n",
        "_classes = None\n",
        "\n",
        "def _load():\n",
        "    global _model, _classes\n",
        "    if _model is None:\n",
        "        _model = tf.keras.models.load_model(MODEL_PATH, compile=False)\n",
        "    if _classes is None:\n",
        "        with open(CLASSES_PATH, \"r\", encoding=\"utf8\") as f:\n",
        "            _classes = json.load(f)\n",
        "    return _model, _classes\n",
        "\n",
        "IMG_SIZE = (224, 224)\n",
        "\n",
        "def preprocess(img_path):\n",
        "    img = image.load_img(img_path, target_size=IMG_SIZE)\n",
        "    arr = image.img_to_array(img) / 255.0\n",
        "    return np.expand_dims(arr, axis=0)\n",
        "\n",
        "def predict_disease(img_path, top_k=3):\n",
        "    model, classes = _load()\n",
        "    arr = preprocess(img_path)\n",
        "    preds = model.predict(arr, verbose=0)[0]\n",
        "    idxs = preds.argsort()[::-1][:top_k]\n",
        "    results = []\n",
        "    for i in idxs:\n",
        "        label = classes[i] if i < len(classes) else str(i)\n",
        "        results.append({\"label\": label, \"confidence\": float(preds[i])})\n",
        "    return {\"predicted_label\": results[0][\"label\"], \"top_k\": results, \"inference_time_ms\": None}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import sys, time\n",
        "    if len(sys.argv) < 2:\n",
        "        print(\"Usage: python disease_tool.py /path/to/image.jpg\")\n",
        "        sys.exit(1)\n",
        "    img = sys.argv[1]\n",
        "    t0 = time.perf_counter()\n",
        "    out = predict_disease(img, top_k=5)\n",
        "    t1 = time.perf_counter()\n",
        "    out[\"inference_time_ms\"] = (t1-t0)*1000.0\n",
        "    print(json.dumps(out, indent=2))\n",
        "'''.strip()\n",
        "\n",
        "# fill in paths safely\n",
        "code = code.replace(\"{MODEL_PATH}\", MODEL_FILE.replace(\"\\\\\", \"\\\\\\\\\"))\n",
        "code = code.replace(\"{CLASSES_PATH}\", CLASSES_FILE.replace(\"\\\\\", \"\\\\\\\\\"))\n",
        "\n",
        "with open(OUT_PATH, \"w\", encoding=\"utf8\") as f:\n",
        "    f.write(code)\n",
        "\n",
        "print(\"Wrote\", OUT_PATH)\n",
        "print(\"You can download this file and put it into your repo's src/ folder.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNGrdC5EZeFe",
        "outputId": "9305e236-3ad2-4149-cb89-4055df73378d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote /content/disease_tool.py\n",
            "You can download this file and put it into your repo's src/ folder.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "\n",
        "MODELS_DIR = \"/content/drive/MyDrive/Buildable-ML-DL-Fellowship/models\"\n",
        "\n",
        "# Files we need\n",
        "files_to_collect = [\n",
        "    \"disease_efficientnet_final.h5\",\n",
        "    \"disease_efficientnet_head_best.h5\",\n",
        "    \"disease_efficientnet_ft_best.h5\",\n",
        "    \"classes_unified.json\",\n",
        "    \"disease_inference_metrics.json\",\n",
        "    \"disease_classification_report_full.txt\",\n",
        "    \"disease_classification_report_test_present.txt\",\n",
        "]\n",
        "\n",
        "# Confusion matrices (all PNGs)\n",
        "pngs = [f for f in os.listdir(MODELS_DIR) if f.endswith(\".png\")]\n",
        "files_to_collect.extend(pngs)\n",
        "\n",
        "# Add your disease_tool.py inside Colab if created\n",
        "if os.path.exists(\"/content/disease_tool.py\"):\n",
        "    files_to_collect.append(\"/content/disease_tool.py\")\n",
        "\n",
        "# ZIP path\n",
        "zip_path = \"/content/final_artifacts.zip\"\n",
        "with zipfile.ZipFile(zip_path, \"w\") as zipf:\n",
        "    for f in files_to_collect:\n",
        "        full_path = f if f.startswith(\"/\") else os.path.join(MODELS_DIR, f)\n",
        "        if os.path.exists(full_path):\n",
        "            zipf.write(full_path, os.path.basename(full_path))\n",
        "\n",
        "# Download ZIP\n",
        "files.download(zip_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "UVB_acraZJe4",
        "outputId": "a3e68ca1-b24c-40e2-c00c-da1c4728770a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ee1309ee-5214-4ca5-8df4-dd95919fe1d4\", \"final_artifacts.zip\", 127651979)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Robust Cell 0 — mount Drive, search for dataset (zip or folder), unzip if needed\n",
        "from google.colab import drive\n",
        "import os, shutil, zipfile, sys, glob, textwrap\n",
        "\n",
        "# 1) mount Drive (safe even if already mounted)\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "DRIVE_BASE = '/content/drive/MyDrive/Buildable-ML-DL-Fellowship'\n",
        "FALLBACKS = [\n",
        "    '/content/plant_disease_fixed',\n",
        "    '/content/plant_disease',\n",
        "    DRIVE_BASE + '/plant_disease',\n",
        "    DRIVE_BASE + '/plant_disease.zip',\n",
        "    DRIVE_BASE + '/data/processed/plant_disease',\n",
        "    DRIVE_BASE + '/data/plant_disease',\n",
        "    DRIVE_BASE + '/data/plant_disease.zip',\n",
        "    DRIVE_BASE + '/plant_disease.zip',\n",
        "    DRIVE_BASE + '/data',\n",
        "    '/content'\n",
        "]\n",
        "\n",
        "def find_dataset():\n",
        "    # look for unzipped folders first\n",
        "    candidates = []\n",
        "    for p in FALLBACKS:\n",
        "        if p.endswith('.zip'):\n",
        "            if os.path.exists(p):\n",
        "                candidates.append(('zip', p))\n",
        "        else:\n",
        "            if os.path.isdir(p):\n",
        "                # check if it contains train/val/test or a folder of classes\n",
        "                subdirs = os.listdir(p)\n",
        "                # quick heuristic\n",
        "                if any(x in subdirs for x in ['train','val','test']) or len(subdirs) > 0:\n",
        "                    candidates.append(('dir', p))\n",
        "    # additional: search Drive_BASE recursively for any folder named plant_disease or plant_disease_fixed\n",
        "    if os.path.isdir(DRIVE_BASE):\n",
        "        for name in ['plant_disease', 'plant_disease_fixed', 'data/processed/plant_disease', 'data/plant_disease']:\n",
        "            full = os.path.join(DRIVE_BASE, name)\n",
        "            if os.path.isdir(full):\n",
        "                candidates.append(('dir', full))\n",
        "        # find zip files at top level matching plant_disease*\n",
        "        zips = glob.glob(os.path.join(DRIVE_BASE, 'plant_disease*.zip')) + glob.glob(os.path.join(DRIVE_BASE, 'data','plant_disease*.zip'))\n",
        "        for z in zips:\n",
        "            candidates.append(('zip', z))\n",
        "\n",
        "    # dedupe preserving order\n",
        "    seen = set()\n",
        "    out = []\n",
        "    for t,p in candidates:\n",
        "        if p not in seen:\n",
        "            out.append((t,p)); seen.add(p)\n",
        "    return out\n",
        "\n",
        "found = find_dataset()\n",
        "\n",
        "if found:\n",
        "    print(\"Found candidate datasets:\")\n",
        "    for t,p in found:\n",
        "        print(f\" - {t}: {p}\")\n",
        "    # Prefer a directory that already contains train/val/test\n",
        "    chosen = None\n",
        "    for t,p in found:\n",
        "        if t == 'dir':\n",
        "            # prefer one that contains train/val/test\n",
        "            subs = os.listdir(p)\n",
        "            if any(s in subs for s in ['train','val','test']):\n",
        "                chosen = p; break\n",
        "    if chosen is None:\n",
        "        # if no candidate dir with splits, pick the first dir candidate\n",
        "        for t,p in found:\n",
        "            if t == 'dir':\n",
        "                chosen = p; break\n",
        "    if chosen is None:\n",
        "        # if only zips, pick first zip\n",
        "        for t,p in found:\n",
        "            if t == 'zip':\n",
        "                chosen = p; break\n",
        "\n",
        "    # If the chosen item is a zip -> unzip into /content/plant_disease\n",
        "    if chosen.endswith('.zip'):\n",
        "        dest = '/content/plant_disease'\n",
        "        print(\"\\nChosen dataset zip:\", chosen)\n",
        "        print(\"Unzipping to\", dest)\n",
        "        # remove existing dest to avoid mixing\n",
        "        if os.path.exists(dest):\n",
        "            print(\"Removing existing\", dest)\n",
        "            shutil.rmtree(dest)\n",
        "        os.makedirs(dest, exist_ok=True)\n",
        "        with zipfile.ZipFile(chosen, 'r') as z:\n",
        "            z.extractall(dest)\n",
        "        # after unzipping, look for likely inner folder\n",
        "        # often zip contains a folder named 'plant_disease' inside dest\n",
        "        inner = os.listdir(dest)\n",
        "        if len(inner) == 1 and os.path.isdir(os.path.join(dest, inner[0])):\n",
        "            # flatten: use inner folder as DATA_ROOT\n",
        "            DATA_ROOT = os.path.join(dest, inner[0])\n",
        "        else:\n",
        "            DATA_ROOT = dest\n",
        "        print(\"Unzip complete. DATA_ROOT set to\", DATA_ROOT)\n",
        "    else:\n",
        "        DATA_ROOT = chosen\n",
        "        print(\"\\nDATA_ROOT set to\", DATA_ROOT)\n",
        "else:\n",
        "    print(\"No dataset found automatically in expected places.\")\n",
        "    print(\"\\nList of files/folders under DRIVE_BASE (please upload the dataset to Drive at this path):\")\n",
        "    if os.path.isdir(DRIVE_BASE):\n",
        "        for item in sorted(os.listdir(DRIVE_BASE))[:200]:\n",
        "            print(\" -\", item)\n",
        "    else:\n",
        "        print(\" DRIVE_BASE not found:\", DRIVE_BASE)\n",
        "    print(textwrap.dedent(\"\"\"\n",
        "    \\nWhat to do next:\n",
        "    1) If you have the dataset as a zip on your laptop, upload it to Google Drive at:\n",
        "         My Drive/Buildable-ML-DL-Fellowship/plant_disease.zip\n",
        "       Use Colab left-side Files -> Upload, or upload via drive.google.com.\n",
        "\n",
        "    2) Or upload the extracted folder (plant_disease or data/processed/plant_disease) into:\n",
        "         My Drive/Buildable-ML-DL-Fellowship/\n",
        "\n",
        "    3) After upload, re-run this cell. The code will detect the zip/folder and set DATA_ROOT automatically.\n",
        "\n",
        "    4) If you prefer to upload directly into Colab, you can use:\n",
        "         from google.colab import files\n",
        "         files.upload()\n",
        "       and then unzip the uploaded file into /content (but Drive is recommended so you keep files persistent).\n",
        "    \"\"\"))\n",
        "    raise FileNotFoundError(\"Dataset not found. Please upload dataset to Drive or Colab and re-run.\")\n",
        "# show a few sample paths for confirmation\n",
        "print(\"\\nSample structure under DATA_ROOT (first 40 entries):\")\n",
        "for root, dirs, files in os.walk(DATA_ROOT):\n",
        "    print(root)\n",
        "    for i,d in enumerate(dirs[:10]): print(\"  dir:\", d)\n",
        "    for i,f in enumerate(files[:10]): print(\"  file:\", f)\n",
        "    break\n",
        "\n",
        "# expose variables for later cells\n",
        "print(\"\\nFinal DATA_ROOT =\", DATA_ROOT)\n",
        "PRUNED_DIR = '/content/pruned_plant_disease'\n",
        "MODELS_DIR = DRIVE_BASE + '/models'\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhGnt8GXQU-n",
        "outputId": "2afb555f-a740-4565-b99c-adaed5caddd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Found candidate datasets:\n",
            " - zip: /content/drive/MyDrive/Buildable-ML-DL-Fellowship/plant_disease.zip\n",
            " - dir: /content\n",
            "\n",
            "DATA_ROOT set to /content\n",
            "\n",
            "Sample structure under DATA_ROOT (first 40 entries):\n",
            "/content\n",
            "  dir: .config\n",
            "  dir: drive\n",
            "  dir: sample_data\n",
            "\n",
            "Final DATA_ROOT = /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zoFcsweiQVMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f0OIHDr1QVTQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}